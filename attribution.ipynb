{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import jsonrpclib\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk.data\n",
    "import re\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "r = re.compile(r'[{}]'.format(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name appears in the paragraph before\n",
    "\n",
    "\n",
    "2 alternant lines \n",
    "\n",
    "if the name appears in the previous or following utterance with a pattern such as ',name,'\n",
    "\n",
    "the object of the speech verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salla\n"
     ]
    }
   ],
   "source": [
    "f = open('annotation_speaker')\n",
    "n = 0\n",
    "instance = []\n",
    "temp = []\n",
    "speaker = []\n",
    "p = 0\n",
    "keyposition = []   #the linenum of the paragraph to be treated\n",
    "for line in f:\n",
    "    if n <= 29560:\n",
    "        if '###############' in line:\n",
    "            keyposition.append(p)\n",
    "            current = [temp.pop()]\n",
    "            previous = temp[:]\n",
    "            temp = []\n",
    "            continue\n",
    "        if 'speaker:\t' in line:\n",
    "            speaker.append(line.split('speaker:\t')[1].split('\\r\\n')[0])\n",
    "            instance.append([current,previous,temp])\n",
    "            temp = []\n",
    "            p = 0\n",
    "            continue\n",
    "        if line.replace(' ','') != '\\r\\n':\n",
    "            temp.append(line.replace('\\r\\n',''))\n",
    "            p = p+1\n",
    "    n = n+1\n",
    "    \n",
    "print speaker[-1]\n",
    "\n",
    "\n",
    "f.close()\n",
    "#instance [0,1,2]:\n",
    "#0 : current paragraph (including the object utterance)\n",
    "#1 : 9 previous paragraphs\n",
    "#2 : 5 following paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = './result_parser.json'\n",
    "\n",
    "def treatresultparser(f):\n",
    "    f = open(f)\n",
    "    result = []\n",
    "    for line in f:\n",
    "        if '#######seperator instance' in line:\n",
    "            result.append([])\n",
    "        elif '#######seperator cases' in line:\n",
    "            result[-1].append([])\n",
    "        elif '########seperator paragraphs' in line:\n",
    "            result[-1][-1].append([])\n",
    "        elif '{' in line:\n",
    "            parsetree = []\n",
    "            words = []\n",
    "            dependency = []\n",
    "            pos = [match.end() for match in re.finditer(re.escape('{\"parsetree\": \"'), line)]\n",
    "            pos2 = [match.start() for match in re.finditer(re.escape(')\", \"text\"'), line)]\n",
    "            pos3 = [match.end() for match in re.finditer(re.escape('\"words\": [['), line)]\n",
    "            pos4 = [match.start() for match in re.finditer(re.escape('], \"indexeddependencies\":'), line)]\n",
    "            pos5 = [match.end() for match in re.finditer(re.escape('\"indexeddependencies\": ['), line)]\n",
    "            pos6 = [match.start() for match in re.finditer(re.escape('}, {\"parsetree\":'), line)]\n",
    "            pos6.append(line.index(']}]}'))\n",
    "            for i in range(len(pos)):\n",
    "                parsetree.append(line[pos[i]:pos2[i]+1])\n",
    "                words.append(line[pos3[i]:pos4[i]+1])\n",
    "            for i in range(len(pos5)):\n",
    "                dependency.append(line[pos5[i]:pos6[i]])\n",
    "            result[-1][-1][-1].append(parsetree)\n",
    "            result[-1][-1][-1].append(words)\n",
    "            for i in range(len(dependency)):\n",
    "                if len(dependency[i]) > 2:\n",
    "                    if dependency[i][-2:] == ']]' :\n",
    "                        dependency[i] = dependency[i][:-1]\n",
    "                if dependency[i] == ']':\n",
    "                    dependency[i] = '[]'\n",
    "            result[-1][-1][-1].append(dependency)\n",
    "    f.close()\n",
    "    return result\n",
    "\n",
    "result = treatresultparser(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 7, 6, 16, 83, 9, 25, 3, 42, 53, 82, 62, 143, 109, 35]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countwordperline(instance,num_instance):\n",
    "    numlist = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(instance[num_instance][i])):\n",
    "            new_strs = r.sub(' ',instance[num_instance][i][j])\n",
    "            num = len(new_strs.split())\n",
    "            numlist.append(num)\n",
    "    numlist.insert(keyposition[num_instance]-1, numlist.pop(0))\n",
    "    return numlist\n",
    "            \n",
    "countwordperline(instance,62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertoffset(numlist,linenum,offset,sentence):\n",
    "    temp = sentence[0:offset]\n",
    "    new_strs = r.sub(' ',temp)\n",
    "    num = len(new_strs.split())\n",
    "    pos = 0\n",
    "    for i in range(linenum-1):\n",
    "        pos = pos + numlist[i]\n",
    "    return pos+num+1\n",
    "\n",
    "convertoffset(countwordperline(instance,0),keyposition[0],33,instance[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# no use, we can get the index from the loop number\n",
    "def convertposition(numlist,linenum,position): \n",
    "    temp = position\n",
    "    i = 0 \n",
    "    while temp > 0:\n",
    "        temp = temp - numlist[i]\n",
    "        i = i+1\n",
    "    i = i-1\n",
    "    if i == linenum-1:\n",
    "        return 0,0\n",
    "    elif i>linenum-1:\n",
    "        return 2,i-linenum\n",
    "    else:\n",
    "        return 1,i\n",
    "\n",
    "convertposition(countwordperline(instance,0),keyposition[0],669)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def infodependency(name,offset,ni,nc,np):\n",
    "    new_strs = r.sub(' ',instance[ni][nc][np])\n",
    "    num = len(new_strs.split())\n",
    "    part = tokenizer.tokenize(instance[ni][nc][np])\n",
    "    length = []\n",
    "    t = len(part)-1\n",
    "    i = 0\n",
    "    while i <= t:\n",
    "        if len(part[i]) > 3:\n",
    "            if part[i][-3:] == '...' and i < t:\n",
    "                part[i] = part[i] +' '+ part[i+1]\n",
    "                part.pop(i+1)\n",
    "                i -= 1\n",
    "                t -= 1\n",
    "                continue\n",
    "            if part[i][:3] == 'And':\n",
    "                part[i-1] = part[i-1] +' '+ part[i]\n",
    "                part.pop(i)\n",
    "                i -= 1\n",
    "                t -= 1\n",
    "                continue\n",
    "        i += 1\n",
    "    for i in range(len(part)):\n",
    "        if i == 0:\n",
    "            length.append(len(part[i]))\n",
    "        elif i > 0:\n",
    "            part[i] = ' '+part[i]\n",
    "            length.append(len(part[i]))\n",
    "\n",
    "    s = 0\n",
    "    linenum = 0\n",
    "    offset_new = 0\n",
    "    for i in range(len(length)):\n",
    "        s += length[i]\n",
    "        if offset - s < 0:\n",
    "            linenum = i\n",
    "            offset_new = offset - length[i-1]\n",
    "            break\n",
    "    key = name+'-'+str(len(re.findall(r\"[\\w]+|[^\\s\\w]\", part[linenum][:offset_new]))+1)\n",
    "    temp = ast.literal_eval(result[ni][nc][np][2][linenum])\n",
    "    head = []\n",
    "    for obj in temp:\n",
    "        if key in obj:\n",
    "            if obj[0] == 'det' or obj[0] == 'amod':\n",
    "                head.append(obj[2].split('-'))\n",
    "    head.sort(key=itemgetter(1))\n",
    "    complete = ''\n",
    "    for obj in head:\n",
    "        complete += obj[0]+' '\n",
    "    complete += name\n",
    "    return complete\n",
    "    \n",
    "infodependency('lady',49,116,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this offset global contains newline\n",
    "def convertoffsetglobal(offsetlocal,ni,nc,nl,instance):\n",
    "    if nc == 0:\n",
    "        for k in range(len(instance[ni][1])):\n",
    "            offsetlocal += len(instance[ni][1][k]) +1\n",
    "    elif nc == 1:\n",
    "        for k in range(nl):\n",
    "            offsetlocal += len(instance[ni][1][k]) +1\n",
    "    elif nc == 2:\n",
    "        for k in range(len(instance[ni][1])):\n",
    "            offsetlocal += len(instance[ni][1][k]) +1\n",
    "        for k in range(nl):\n",
    "            offsetlocal += len(instance[ni][2][k]) +1\n",
    "        offsetlocal += len(instance[ni][0][0]) +1\n",
    "    return offsetlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ner(instance,result,keyposition):\n",
    "    entities = []\n",
    "    n = 0\n",
    "    for obj in instance:\n",
    "        numlist = countwordperline(instance,n)\n",
    "        current = [[]]\n",
    "        temp = []\n",
    "        for z in range(len(result[n][0][0][0])):\n",
    "            temp = temp + result[n][0][0][0][z].split('] [')\n",
    "        for frag in temp:\n",
    "            if 'PartOfSpeech=NNP' in frag or 'NamedEntityTag=PERSON' in frag:\n",
    "                text = frag.split('Text=')[1].split(' ')[0]\n",
    "                position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                current[0].append([text,position,pos,offset,'PERSON'])\n",
    "            elif 'PartOfSpeech=N' in frag:\n",
    "                text = frag.split('Text=')[1].split(' ')[0]\n",
    "                position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                try:\n",
    "                    pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                except IndexError:\n",
    "                    print \n",
    "                    ccccccc\n",
    "                lex = []\n",
    "                for synset in wn.synsets(text):\n",
    "                    lex.append(synset.lexname())\n",
    "                lenlex = 0\n",
    "                for c in lex:\n",
    "                    if 'noun' in c:\n",
    "                        lenlex += 1\n",
    "                if lex == []:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "                elif lenlex == 0:\n",
    "                    continue\n",
    "                elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "        temp = []\n",
    "        for z in range(len(result[n][0][0][1])):\n",
    "            temp = temp + result[n][0][0][1][z].split('] [')\n",
    "        for frag in temp:\n",
    "            part = frag.split('\"')\n",
    "            if part[part.index('PartOfSpeech')+2] == 'NNP' or part[part.index('NamedEntityTag')+2] == 'PERSON':\n",
    "                text = frag.split('\"')[1]\n",
    "                position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                current[0].append([text,position,pos,offset,'PERSON'])\n",
    "            elif 'N' in part[part.index('PartOfSpeech')+2]:\n",
    "                text = frag.split('\"')[1]\n",
    "                position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                lex = []\n",
    "                for synset in wn.synsets(text):\n",
    "                    lex.append(synset.lexname())\n",
    "                lenlex = 0\n",
    "                for c in lex:\n",
    "                    if 'noun' in c:\n",
    "                        lenlex += 1\n",
    "                if lex == []:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "                elif lenlex == 0:\n",
    "                    continue\n",
    "                elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "        previous = []\n",
    "        for i in range(len(obj[1])):\n",
    "            previous.append([])\n",
    "            temp = []\n",
    "            for x in range(len(result[n][1][i][0])):\n",
    "                try:\n",
    "                    temp = temp + result[n][1][i][0][x].split('] [')\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for frag in temp:\n",
    "                if 'PartOfSpeech=NNP' in frag or 'NamedEntityTag=PERSON' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    previous[i].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'PartOfSpeech=N' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "            temp = []\n",
    "            for x in range(len(result[n][1][i][1])):\n",
    "                temp = temp + result[n][1][i][1][x].split('] [')\n",
    "            for frag in temp:\n",
    "                part = frag.split('\"')\n",
    "                if part[part.index('PartOfSpeech')+2] == 'NNP' or part[part.index('NamedEntityTag')+2] == 'PERSON':\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    previous[i].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'N' in part[part.index('PartOfSpeech')+2]:\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "        following = []\n",
    "        for j in range(len(obj[2])):\n",
    "            following.append([])\n",
    "            temp = []\n",
    "            for y in range(len(result[n][2][j][0])):\n",
    "                try:\n",
    "                    temp = temp + result[n][2][j][0][y].split('] [')\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for frag in temp:\n",
    "                if 'PartOfSpeech=NNP' in frag or 'NamedEntityTag=PERSON' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    following[j].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'PartOfSpeech=N' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "            temp = []\n",
    "            for y in range(len(result[n][2][j][1])):\n",
    "                temp = temp + result[n][2][j][1][y].split('] [')\n",
    "            for frag in temp:\n",
    "                part = frag.split('\"')\n",
    "                if part[part.index('PartOfSpeech')+2] == 'NNP' or part[part.index('NamedEntityTag')+2] == 'PERSON':\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    following[j].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'N' in part[part.index('PartOfSpeech')+2]:\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(len(lex)) >= 0.4:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "        entities.append([current,previous,following])\n",
    "        n = n +1\n",
    "    return entities\n",
    "\n",
    "entities = ner(instance,result,keyposition)\n",
    "for i in range(len(entities)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities[i][j])):\n",
    "            entities[i][j][k].sort(key=itemgetter(2))\n",
    "entities_new = entities[:]\n",
    "\n",
    "#entities [name, offset in the paragraph, word position in the instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine contiguous name entities and separate those within and outside quotation marks\n",
    "def contiguous(instance,namelist,ni,cls,nl):\n",
    "    i = 0\n",
    "    t = len(namelist)-1\n",
    "    newlist = {}\n",
    "    newlist['in'] = []\n",
    "    newlist['out'] = []\n",
    "    while i < t:\n",
    "        if namelist[i+1][1]-namelist[i][1] == len(namelist[i][0])+1 and instance[ni][cls][nl][namelist[i][1]+len(namelist[i][0])] == ' ' and namelist[i+1][4] == namelist[i][4]:\n",
    "            namelist[i][0] = namelist[i][0]+' '+namelist[i+1][0]\n",
    "            namelist.pop(i+1)\n",
    "            t = t - 1\n",
    "            i = i - 1\n",
    "        i = i + 1\n",
    "    for obj in namelist:\n",
    "        if instance[ni][cls][nl][0:obj[1]].count('\"')%2 == 1:\n",
    "            newlist['in'].append(obj)\n",
    "        else:\n",
    "            newlist['out'].append(obj)\n",
    "    return newlist\n",
    "\n",
    "for i in range(len(entities)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(instance[i][j])):\n",
    "            entities_new[i][j][k] = contiguous(instance,entities[i][j][k],i,j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findentities_out(entities_new):\n",
    "    entities_out = []\n",
    "    entities_in = []\n",
    "    for i in range(len(entities_new)):\n",
    "        entities_out.append([])\n",
    "        entities_in.append([])\n",
    "        for j in range(3):\n",
    "            entities_out[i].append([])\n",
    "            entities_in[i].append([])\n",
    "            for k in range(len(entities_new[i][j])):\n",
    "                if entities_new[i][j][k] == {}:\n",
    "                    entities_out[i][j].append([])\n",
    "                    entities_in[i][j].append([])\n",
    "                else:\n",
    "                    entities_out[i][j].append([])\n",
    "                    entities_out[i][j][k] = entities_new[i][j][k]['out']\n",
    "                    entities_in[i][j].append([])\n",
    "                    entities_in[i][j][k] = entities_new[i][j][k]['in']\n",
    "    return entities_out,entities_in\n",
    "\n",
    "entities_out,entities_in = findentities_out(entities_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#####only for noting who is the speaker from the candidates\n",
    "i = 0\n",
    "#print instance[i]\n",
    "fact = []\n",
    "for i in range(len(instance)):\n",
    "    print instance[i][0][0]\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities_out[i][j])):\n",
    "            for l in range(len(entities_out[i][j][k])):\n",
    "                print entities_out[i][j][k][l], str(j), str(k), str(l) + '\\n'\n",
    "    fact.append(input(\"which : \"))\n",
    "#print result[i][0][0][2][1]\n",
    "print fact\n",
    "####maybe I should add those adj and det and see if it improve the result######\n",
    "#####The Tattered Prince#####\n",
    "#####the High Septon######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('./fact')\n",
    "fact = []\n",
    "for line in f:\n",
    "    fact = ast.literal_eval(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###is there any quote in the paragraph which this person shows\n",
    "###how many quotes between this person and the target quote\n",
    "###all the index of paragraphs in which this person shows\n",
    "###the last puctuation marks of the last quote and is there a close quotation mark\n",
    "###whether this is a name entity or a personal noun\n",
    "###whether this person is a object of a speech verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "notfound = []\n",
    "for i in range(len(entities_out)):\n",
    "    found = 0\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities_out[i][j])):\n",
    "            for obj in entities_out[i][j][k]:\n",
    "                if speaker[i] == obj[0]:\n",
    "                    found = 1\n",
    "    if found == 0:\n",
    "        notfound.append(i)\n",
    "print notfound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countpunctuation(sentence,l):\n",
    "    punctuation = [',', '.', '\\n', '!', '?', '\"']\n",
    "    for i in range(len(punctuation)):\n",
    "        l[i] = l[i] + sentence.count(punctuation[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to extract the distance between the candidat and the quote, with counting the punctuations between them\n",
    "def distance(instance,entities,num_instance,keyposition):\n",
    "    sentence = instance[num_instance][0][0]\n",
    "    numlist = countwordperline(instance,num_instance)\n",
    "    if sentence.count('\"') == 1:\n",
    "        offset = [sentence.index('\"')]\n",
    "        position = [convertoffset(countwordperline(instance,num_instance),keyposition[num_instance],offset[0],instance[num_instance][0][0])]\n",
    "    else:\n",
    "        offset = []\n",
    "        p = 0\n",
    "        for c in sentence:\n",
    "            if c == '\"':\n",
    "                offset.append(p)\n",
    "            p = p+1\n",
    "        position = []\n",
    "        for i in range(2):\n",
    "            position.append(convertoffset(countwordperline(instance,num_instance),keyposition[num_instance],offset[i],instance[num_instance][0][0]))\n",
    "    dist = []\n",
    "    cp = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities[num_instance][i])):\n",
    "            for k in range(len(entities[num_instance][i][j])):\n",
    "                cp.append([0,0,0,0,0,0])\n",
    "                pos = entities[num_instance][i][j][k][2]\n",
    "                ofs = entities[num_instance][i][j][k][1]\n",
    "                if len(position) == 1:\n",
    "                    dist.append([abs(pos-position[0])])\n",
    "                else:\n",
    "                    if pos <= position[0]:\n",
    "                        dist.append([position[0]-pos])\n",
    "                    elif pos >= position[1]:\n",
    "                        dist.append([pos-position[1]])\n",
    "                stc = instance[num_instance][i][j]\n",
    "                if len(offset)>1:\n",
    "                    if i == 1:\n",
    "                        countpunctuation(stc[ofs:],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                        for s in range(j+1,keyposition[num_instance]-1):\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                        countpunctuation(sentence[:offset[0]],cp[-1])\n",
    "                    elif (i == 0 and ofs < offset[0]):\n",
    "                        countpunctuation(sentence[ofs:offset[0]],cp[-1])\n",
    "                    elif i == 2:\n",
    "                        countpunctuation(stc[:ofs],cp[-1])\n",
    "                        for s in range(j):\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                        countpunctuation(sentence[offset[1]+1:],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                    else:\n",
    "                        countpunctuation(sentence[offset[1]+1:ofs+1],cp[-1])\n",
    "                else:\n",
    "                    if i == 1:\n",
    "                        countpunctuation(stc[ofs:],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                        for s in range(j+1,keyposition[num_instance]-1):\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                        countpunctuation(sentence[:offset[0]],cp[-1])\n",
    "                    elif i == 2:\n",
    "                        countpunctuation(stc[:ofs],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                        for s in range(j):\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                    else:\n",
    "                        countpunctuation(sentence[ofs:offset[0]],cp[-1])\n",
    "                        \n",
    "    return dist,cp\n",
    "\n",
    "dst = []\n",
    "feature = []\n",
    "order = []\n",
    "for i in range(len(entities_out)):\n",
    "    dst.append(distance(instance,entities_out,i,keyposition))\n",
    "    feature.append(dst[i][0][:])\n",
    "    order.append(feature[i][:])\n",
    "    temp = sorted(range(len(order[i])), key=lambda k: order[i][k])\n",
    "    n = 0\n",
    "    for obj in temp:\n",
    "        order[i][obj].append(n)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detailparagraph(instance,num_instance,entities,keyposition):\n",
    "    dp = []\n",
    "    keyline = keyposition[num_instance]\n",
    "    offset = 10 - keyline\n",
    "    numlist = countwordperline(instance,num_instance)\n",
    "    for i in range(45):\n",
    "        dp.append(0)\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities[num_instance][i])):\n",
    "            num = instance[num_instance][i][j].count('\"')\n",
    "            num = num/2+num%2\n",
    "            if i == 1:\n",
    "                dp[3*(j+offset)+1] += num\n",
    "            elif i == 0:\n",
    "                dp[28] += num\n",
    "            elif i == 2:\n",
    "                dp[30+3*j+1] += num\n",
    "            for obj in entities[num_instance][i][j]:\n",
    "                if i == 1:\n",
    "                    dp[3*(j+offset)] += 1\n",
    "                elif i == 0:\n",
    "                    dp[27] += 1\n",
    "                elif i == 2:\n",
    "                    dp[30+3*j] += 1\n",
    "    for i in range(len(numlist)):\n",
    "        dp[2+3*(i+offset)] = numlist[i]\n",
    "    return dp\n",
    "                \n",
    "dp = []\n",
    "for i in range(len(entities_out)):\n",
    "    dp.append(detailparagraph(instance,i,entities_out,keyposition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findsimilarname(entities_i):\n",
    "    names = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities_i[i])):\n",
    "            for obj in entities_i[i][j]:\n",
    "                names.append(obj+[i,j])\n",
    "                \n",
    "    for obj in names:\n",
    "        if obj[0].find(' ') == -1:\n",
    "            bad = 0\n",
    "            for obj2 in names:\n",
    "                if obj[0].lower() in obj2[0].lower():\n",
    "                    for obj3 in names:\n",
    "                        if obj[0].lower() in obj3[0].lower():\n",
    "                            part2 = obj2[0].lower().split(' ')\n",
    "                            part3 = obj3[0].lower().split(' ')\n",
    "                            not_match = 0\n",
    "                            for t in part2:\n",
    "                                if t not in part3 and part2.index(t) != 0:\n",
    "                                    not_match = 1\n",
    "                            for t in part3:\n",
    "                                if t not in part2 and not_match == 1 and part3.index(t) != 0:\n",
    "                                    bad = 1\n",
    "            if bad == 1:\n",
    "                names.remove(obj)         \n",
    "                            \n",
    "    \n",
    "    names_new = []\n",
    "    for obj in names:\n",
    "        temp = obj[0].split(' ')\n",
    "        for obj2 in names:\n",
    "            if obj != obj2:\n",
    "                for t in temp:\n",
    "                    if ' '+t.lower()+' ' in ' '+obj2[0].lower()+' ' and ((temp.index(t) != 0  and len(temp) > 1) or (len(temp) == 1)):\n",
    "                        tmp = obj2[0].lower().split(' ')\n",
    "                        if len(temp) == 2 and len(tmp) == 2 and (t.lower() == tmp[1] and tmp[0] not in [x.lower() for x in temp]):\n",
    "                            continue\n",
    "                        else:\n",
    "                            found = 0\n",
    "                            for i in range(len(names_new)):\n",
    "                                if obj in names_new[i] and obj2 not in names_new[i]:\n",
    "                                    names_new[i].append(obj2)\n",
    "                                    found = 1\n",
    "                                elif obj2 in names_new[i] and obj not in names_new[i]:\n",
    "                                    names_new[i].append(obj)\n",
    "                                    found = 1\n",
    "                                elif obj2 in names_new[i] and obj in names_new[i]:\n",
    "                                    found = 1\n",
    "                                    pass\n",
    "                            if found == 0:\n",
    "                                names_new.append([obj,obj2])\n",
    "    for obj in names_new:\n",
    "        for obj2 in obj:\n",
    "            for obj3 in names_new:\n",
    "                if obj != obj3:\n",
    "                    if obj2 in obj3:\n",
    "                        if len(obj) >= len(obj3):\n",
    "                            names_new.remove(obj3)\n",
    "                        else:\n",
    "                            try:\n",
    "                                names_new.remove(obj)\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "    return names_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nameoccurrence(entities,keyposition):\n",
    "    nocc = []\n",
    "    for k in range(len(entities)):\n",
    "        nocc.append([])\n",
    "        names_new = findsimilarname(entities[k])\n",
    "        key = keyposition[k]\n",
    "        filling = 10 - key\n",
    "        for m in range(len(names_new)):\n",
    "            pl = []\n",
    "            for l in range(15):\n",
    "                pl.append(0)\n",
    "            for obj2 in names_new[m]:\n",
    "                if obj2[-2] == 0:\n",
    "                    pl[9] += 1\n",
    "                elif obj2[-2] == 1:\n",
    "                    pl[obj2[-1]+filling] += 1\n",
    "                elif obj2[-2] == 2:\n",
    "                    pl[10+obj2[-1]] += 1\n",
    "            names_new[m].append(pl)\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(len(entities[k][i])):\n",
    "                for obj in entities[k][i][j]:\n",
    "                    num = 1\n",
    "                    for obj2 in names_new:\n",
    "                        if obj+[i,j] in obj2:\n",
    "                            num = len(obj2)-1\n",
    "                            res = [num]+obj2[-1]\n",
    "                    if num == 1:\n",
    "                        pl = []\n",
    "                        for l in range(15):\n",
    "                            pl.append(0)\n",
    "                        if i == 0:\n",
    "                            pl[9] += 1\n",
    "                        elif i == 1:\n",
    "                            pl[j+filling] += 1\n",
    "                        elif i == 2:\n",
    "                            pl[10+j] += 1\n",
    "                        res = [num]+pl\n",
    "                    nocc[-1].append(res)\n",
    "    return nocc\n",
    "\n",
    "nocc = nameoccurrence(entities_out,keyposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### the next part is for treating the target text ####\n",
    "f = open('./target_text')\n",
    "n = 0\n",
    "instance_target = []\n",
    "temp = []\n",
    "p = 0\n",
    "keyposition_target = []\n",
    "for line in f:\n",
    "    if n <= 4400:\n",
    "        if '###############' in line:\n",
    "            keyposition_target.append(p)\n",
    "            current = [temp.pop()]\n",
    "            previous = temp[:]\n",
    "            temp = []\n",
    "            continue\n",
    "        if 'speaker:' in line:\n",
    "            instance_target.append([current,previous,temp])\n",
    "            temp = []\n",
    "            p = 0\n",
    "            continue\n",
    "        if line.replace(' ','') != '\\r\\n':\n",
    "            temp.append(line.replace('\\r\\n',''))\n",
    "            p = p+1\n",
    "    n = n+1\n",
    "    \n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "result_target = treatresultparser('./result_parser_target.json')\n",
    "entities_target = ner(instance_target,result_target,keyposition_target)\n",
    "for i in range(len(entities_target)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities_target[i][j])):\n",
    "            entities_target[i][j][k].sort(key=itemgetter(2))\n",
    "entities_target_new = entities_target[:]\n",
    "for i in range(len(entities_target)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(instance_target[i][j])):\n",
    "            entities_target_new[i][j][k] = contiguous(instance_target,entities_target[i][j][k],i,j,k)\n",
    "entities_target_out,entities_target_in = findentities_out(entities_target_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word,CharacterOffsetBegin,PartOfSpeech,NamedEntityTag,Lemma,case,line\n",
    "def analyseparser(result,num_instance,instance,entities):\n",
    "    POS = []\n",
    "    for i in [1,0,2]:\n",
    "        for j in range(len(result[num_instance][i])):\n",
    "            for k in range(len(result[num_instance][i][j][1])):\n",
    "                temp = []\n",
    "                entity = result[num_instance][i][j][1][k].split('\"')[1]\n",
    "                info = ast.literal_eval(result[num_instance][i][j][1][k].split(', ',1)[1].replace(']',''))\n",
    "                offset = convertoffsetglobal(int(info['CharacterOffsetBegin']),num_instance,i,j,instance)\n",
    "                temp = [entity,offset,info['PartOfSpeech'],info['NamedEntityTag'],info['Lemma']]\n",
    "                POS.append(temp)\n",
    "                temp = []\n",
    "                part = result[num_instance][i][j][0][k].split('] [')\n",
    "                for l in range(len(part)):\n",
    "                    info = part[l].split(' ')\n",
    "                    entity = ''\n",
    "                    CharacterOffsetBegin = ''\n",
    "                    PartOfSpeech = ''\n",
    "                    Lemma = ''\n",
    "                    NamedEntityTag = ''\n",
    "                    for obj in info:\n",
    "                        if '[Text=' in obj:\n",
    "                            entity = obj[6:]\n",
    "                        elif 'Text=' in obj:\n",
    "                            entity = obj[5:]\n",
    "                        if 'CharacterOffsetBegin=' in obj:\n",
    "                            CharacterOffsetBegin = int(obj[21:])\n",
    "                        if 'PartOfSpeech=' in obj:\n",
    "                            PartOfSpeech = obj[13:]\n",
    "                        if 'Lemma=' in obj:\n",
    "                            Lemma = obj[6:]\n",
    "                        if 'NamedEntityTag=' in obj:\n",
    "                            NamedEntityTag = obj[15:]\n",
    "                    if CharacterOffsetBegin == '':\n",
    "                        continue\n",
    "                    offset = convertoffsetglobal(CharacterOffsetBegin,num_instance,i,j,instance)\n",
    "                    temp = [entity,offset,PartOfSpeech,NamedEntityTag,Lemma]\n",
    "                    POS.append(temp)\n",
    "    return POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature: 5 words near the candidate: 1 expression verb, 2 punctuation mark, 3 person, 4 newline, 5 nothing, 0 other cases\n",
    "def neighbourhood(instance,num_instance,entities,result):\n",
    "    f = open('./speechverbs')\n",
    "    speechverb = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            speechverb.append(line.split('\\n')[0].lower())\n",
    "    DISTANCE = 5\n",
    "    text = ''\n",
    "    nb = []\n",
    "    for i in [1,0,2]:\n",
    "        for obj in instance[num_instance][i]:\n",
    "            text += obj+'\\n'\n",
    "    analyse = analyseparser(result,num_instance,instance,entities)\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities[num_instance][i])):\n",
    "            for obj in entities[num_instance][i][j]:\n",
    "                temp = []\n",
    "                offset = obj[1]\n",
    "                offset = convertoffsetglobal(offset,num_instance,i,j,instance)\n",
    "                list1 = [(m.group(0), m.start(0)) for m in re.finditer(r\"[\\w]+|[.]+|[\\w]+[-][\\w]+|[\"+string.punctuation+\"]|[\\n]\", text[:offset])]\n",
    "                list2 = [(m.group(0), m.start(0)+offset) for m in re.finditer(r\"[\\w]+|[.]+|[\\w]+[-][\\w]+|[\"+string.punctuation+\"]|[\\n]\", text[offset:])] \n",
    "                if len(list1) >= DISTANCE:\n",
    "                    previous = list1[-DISTANCE:]\n",
    "                elif len(list1) < DISTANCE:\n",
    "                    previous = list1[-len(list1):]\n",
    "                if len(list2) > DISTANCE:\n",
    "                    following = list2[1:DISTANCE+1]\n",
    "                elif len(list2) <= DISTANCE:\n",
    "                    following = list2[1:len(list2)]\n",
    "                filling = DISTANCE-len(previous)\n",
    "                for k in range(filling):\n",
    "                    temp.append(5)\n",
    "                for part in previous + following:\n",
    "                    writen = 0\n",
    "                    if part[0] == '\\n':\n",
    "                        temp.append(4)\n",
    "                        writen = 1\n",
    "                    num = 0\n",
    "                    for c in part[0]:\n",
    "                        if c in string.punctuation:\n",
    "                            num += 1\n",
    "                    if num == len(part[0]) and writen != 1:\n",
    "                        temp.append(2)\n",
    "                        writen = 1\n",
    "                    for elem in analyse:\n",
    "                        if elem[1] == part[1] and elem[0] == part[0]:\n",
    "                            if writen != 1 and (elem[2] == 'NNP' or elem[3] == 'PERSON'):\n",
    "                                temp.append(3)\n",
    "                                writen = 1\n",
    "                            if writen != 1 and ('VB' in elem[2] and elem[4].lower() in speechverb):\n",
    "                                temp.append(1)\n",
    "                                writen = 1\n",
    "                        elif elem[1] == part[1] and elem[0] != part[0]:\n",
    "                            pass\n",
    "                    if writen == 0:\n",
    "                        temp.append(0)\n",
    "                nb.append(temp)\n",
    "                filling = DISTANCE-len(following)\n",
    "                for k in range(filling):\n",
    "                    temp.append(5)\n",
    "    f.close()\n",
    "    return nb\n",
    "\n",
    "nb = []\n",
    "for i in range(len(instance)):            \n",
    "    nb.append(neighbourhood(instance,i,entities_out,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countentities(instance,entities):\n",
    "    ce = []\n",
    "    for i in range(len(entities)):\n",
    "        ce.append([])\n",
    "        offset_quote = convertoffsetglobal(instance[i][0][0].index('\"'),i,0,0,instance)\n",
    "        for j in range(3):\n",
    "            for k in range(len(entities[i][j])):\n",
    "                for obj in entities[i][j][k]:\n",
    "                    num = 0\n",
    "                    offset = obj[3]\n",
    "                    ce[-1].append([])\n",
    "                    for l in range(3):\n",
    "                        for m in range(len(entities[i][l])):\n",
    "                            for obj2 in entities[i][l][m]:\n",
    "                                if (obj2[3]-offset)*(obj2[3]-offset_quote) <0:\n",
    "                                    num += 1\n",
    "                    ce[-1][-1].append(num)\n",
    "    return ce\n",
    "\n",
    "ce = countentities(instance,entities_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing: need to combine those quotes with multilines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### for function tokenize\n",
    "def findcase(token,pos,name_similar):\n",
    "    if '<C' in token:\n",
    "        sublist = [int(token[2:-1]),pos]\n",
    "        for obj in name_similar:\n",
    "            temp = 1\n",
    "            for elem in [x in name_similar for x in sublist]:\n",
    "                temp *= elem\n",
    "            if temp == 1:\n",
    "                return 1\n",
    "        return 2\n",
    "    elif '<N' in token:\n",
    "        return 3\n",
    "    elif '<P' in token:\n",
    "        return 4\n",
    "    elif '<Q' in token:\n",
    "        return 5\n",
    "    elif '<TQ' in token:\n",
    "        return 6\n",
    "    elif '<S' in token:\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "p = [1,2,23,4]\n",
    "print p[-1:]\n",
    "print range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(instance,entities,result):\n",
    "    f = open('./speechverbs')\n",
    "    speechverb = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            speechverb.append(line.split('\\n')[0].lower())\n",
    "    exclude = set(string.punctuation)\n",
    "    tk = []\n",
    "    NUM_TOKENS = 3   # number of tokens taken both sides\n",
    "    for i in range(len(instance)):\n",
    "        text = ''\n",
    "        num = 0\n",
    "        ofs = []\n",
    "        temp = 0\n",
    "        for j in [1,0,2]:\n",
    "            for k in range(len(instance[i][j])):\n",
    "                text += instance[i][j][k]+'\\n'\n",
    "                for obj in entities[i][j][k]:\n",
    "                    temp += len(obj[0])-len('<C'+str(num)+'>')\n",
    "                    ofs.append([temp,obj[3]])\n",
    "                    num += 1\n",
    "        quote_start = convertoffsetglobal(instance[i][0][0].index('\"'),i,0,0,instance)\n",
    "        quote_end = min(text[quote_start+1:].index('\\n'),text[quote_start+1:].index('\"'))+quote_start+2\n",
    "        analyse = analyseparser(result,i,instance,entities)\n",
    "        sv = []\n",
    "        for obj in analyse:\n",
    "            if 'V' in obj[2] and obj[4].lower() in speechverb and (int(obj[1]) > quote_end or int(obj[1]) < quote_start):\n",
    "                sv.append(obj)\n",
    "        text = text[:quote_start]+'<TQ>'+text[quote_end:]\n",
    "        offset = quote_end - quote_start - len('<TQ>')\n",
    "        ofs.append([offset,quote_start])\n",
    "        ofs = sorted(ofs,key=itemgetter(1))\n",
    "        for t in range(ofs.index([offset,quote_start])+1,len(ofs)):\n",
    "            ofs[t][0] += offset\n",
    "        ofs.remove([offset,quote_start])\n",
    "        for j in [2,0,1]:\n",
    "            for k in reversed(range(len(entities[i][j]))):\n",
    "                for obj in reversed(entities[i][j][k]):\n",
    "                    name_start = obj[3]\n",
    "                    name_end = obj[3] + len(obj[0])\n",
    "                    if name_start > quote_start:\n",
    "                        name_start -= offset\n",
    "                        name_end -= offset\n",
    "                    text = text[:name_start]+'<C'+str(num-1)+'>'+text[name_end:]\n",
    "                    num -= 1\n",
    "        pos = []\n",
    "        for obj in sv:\n",
    "            filling = 0\n",
    "            for j in range(len(ofs)):\n",
    "                if obj[1] < ofs[0][1]:\n",
    "                    verb_start = obj[1]\n",
    "                    verb_end = obj[1]+len(obj[0])\n",
    "                    pos.append((verb_start,verb_end))\n",
    "                    break\n",
    "                elif obj[1] < ofs[j][1]:\n",
    "                    if ofs[j][1] > quote_end and ofs[j-1][1] < quote_start and obj[1] > quote_end:\n",
    "                        verb_start = obj[1]-ofs[j-1][0] - offset\n",
    "                        verb_end = obj[1]-ofs[j-1][0]+len(obj[0]) - offset\n",
    "                        pos.append((verb_start,verb_end))\n",
    "                        break\n",
    "                    else:\n",
    "                        verb_start = obj[1]-ofs[j-1][0]\n",
    "                        verb_end = obj[1]-ofs[j-1][0]+len(obj[0])\n",
    "                        pos.append((verb_start,verb_end))\n",
    "                        break\n",
    "        for obj in reversed(pos):\n",
    "            text = text[:obj[0]]+'<S>'+text[obj[1]:]\n",
    "        text = text.replace('\\n','<N>')\n",
    "        pos = []\n",
    "        num = 0\n",
    "        locked = 0\n",
    "        mark = 0\n",
    "        for c in text:\n",
    "            if c == '\"' and locked == 0:\n",
    "                locked = 1\n",
    "                start = num\n",
    "            elif c == '\"' and locked == 1:\n",
    "                locked = 0\n",
    "                end = num\n",
    "                pos.append((start,end))\n",
    "            elif c == '<' and locked == 1:\n",
    "                mark = 1\n",
    "            elif c == 'N' and mark == 1:\n",
    "                end = num\n",
    "                locked = 0\n",
    "                mark = 0\n",
    "                pos.append((start,end))\n",
    "            elif mark == 1:\n",
    "                mark = 0\n",
    "            num += 1\n",
    "        for obj in reversed(pos):\n",
    "            text = text[:obj[0]]+'<Q>'+text[obj[1]+1:]\n",
    "        for c in string.punctuation:\n",
    "            if c != '<' and c != '>' and c != '.':\n",
    "                text=text.replace(c,'<P>')\n",
    "        pos = []\n",
    "        num = 0\n",
    "        locked = 0\n",
    "        for c in text:\n",
    "            if c == '<' and locked == 0:\n",
    "                locked = 1\n",
    "                temp = num\n",
    "            elif c == '<' and locked == 1:\n",
    "                pos.append(temp)\n",
    "            elif c == '>' and locked == 1:\n",
    "                locked = 0\n",
    "            elif c == '>' and locked == 0:\n",
    "                pos.append(num)\n",
    "            num += 1\n",
    "        for obj in reversed(pos):\n",
    "            text = text[:obj]+'<P>'+text[obj+1:]\n",
    "        text = re.sub(r\"[.]+\", '<P>', text)\n",
    "        name_similar = findsimilarname(entities[i])\n",
    "        order = []\n",
    "        for j in [1,0,2]:\n",
    "            for obj in entities[i][j]:\n",
    "                order += [x[3] for x in obj]\n",
    "        for j in range(len(name_similar)):\n",
    "            for k in range(len(name_similar[j])):\n",
    "                name_similar[j][k] = order.index(name_similar[j][k][3])\n",
    "        locked = 0\n",
    "        tokens = ''\n",
    "        for ch in text:\n",
    "            if ch == '<':\n",
    "                temp = ''\n",
    "                locked = 1\n",
    "            elif ch == '>':\n",
    "                locked = 0\n",
    "                tokens += '<'+temp+'> '\n",
    "            elif locked == 1:\n",
    "                temp += ch\n",
    "        tokens = tokens.split()\n",
    "        tk.append([])\n",
    "        for j in range(3):\n",
    "            for k in range(len(entities[i][j])):\n",
    "                for obj in entities[i][j][k]:\n",
    "                    temp = []\n",
    "                    pos = order.index(obj[3])\n",
    "                    for l in range(len(tokens)):\n",
    "                        if '<C'+str(pos)+'>' in tokens[l]:\n",
    "                            if l < NUM_TOKENS:\n",
    "                                filling = NUM_TOKENS - l\n",
    "                                for m in range(filling):\n",
    "                                    temp.append(0)\n",
    "                                for m in range(l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,l+NUM_TOKENS+1):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                            elif l+NUM_TOKENS >= len(tokens):\n",
    "                                filling = l+NUM_TOKENS-len(tokens)+1\n",
    "                                for m in range(l-3,l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,len(tokens)):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(filling):\n",
    "                                    temp.append(0)\n",
    "                            elif l < NUM_TOKENS and l+NUM_TOKENS >= len(tokens):\n",
    "                                filling1 = NUM_TOKENS - l\n",
    "                                filling2 = l+NUM_TOKENS-len(tokens)+1\n",
    "                                for m in range(filling1):\n",
    "                                    temp.append(0)\n",
    "                                for m in range(l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,len(tokens)):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(filling2):\n",
    "                                    temp.append(0)\n",
    "                            else:\n",
    "                                for m in range(l-3,l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,l+NUM_TOKENS+1):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                    tk[-1].append(temp)\n",
    "    f.close()\n",
    "    return tk\n",
    "tk = tokenize(instance,entities_out,result)\n",
    "#### <CN> candidate, <N> newline, <P> punctuation, <Q> quote, <TQ> target quote, <S> speech verb\n",
    "#### 0: nothing, 1: the same candidate, 2: other candidates, 3: newline, 4: punctuation, 5: other quote, 6: target quote, 7: speech verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###[1 one quotation mark or not, how many quote in it, 1 whether speaker is mentioned or not,1 whether other speaker is mentioned or not, how far from the start, the length in word]\n",
    "def quotefeature(instance,entities_out,entities_in):\n",
    "    qf = []\n",
    "    for i in range(len(instance)):\n",
    "        qf.append([])\n",
    "        if instance[i][0][0].count('\"') == 1:\n",
    "            num_qm = 1\n",
    "            num_quote = 1\n",
    "        else:\n",
    "            num_quote = instance[i][0][0].count('\"')%2 + instance[i][0][0].count('\"')/2\n",
    "            num_qm = 0\n",
    "        text = instance[i][0][0]\n",
    "        text_temp = re.split(r'[^0-9A-Za-z\\']+',text[:text.index('\"')])\n",
    "        while '' in text_temp:\n",
    "            text_temp.remove('')\n",
    "        distance = len(text_temp)\n",
    "        quote = ''\n",
    "        part = instance[i][0][0].split('\"')\n",
    "        for p in range(len(part)):\n",
    "            if p%2 == 1:\n",
    "                quote += part[p]\n",
    "        text_temp = re.split(r'[^0-9A-Za-z\\']+',quote)\n",
    "        while '' in text_temp:\n",
    "            text_temp.remove('')\n",
    "        length = len(text_temp)\n",
    "        temp_qf = []\n",
    "        entities_mixed = []\n",
    "        entities_extended = []\n",
    "        for j in range(3):\n",
    "            entities_mixed.append([])\n",
    "            for k in range(len(entities_out[i][j])):\n",
    "                entities_mixed[-1].append([])\n",
    "                entities_mixed[-1][-1] = []\n",
    "                for obj in entities_out[i][j][k]:\n",
    "                    entities_extended.append(obj)\n",
    "                    entities_mixed[-1][-1].append(obj+['out'])\n",
    "            if j == 0 and k == 0:\n",
    "                temp = []\n",
    "                for obj in entities_in[i][j][k]:\n",
    "                    temp.append(obj+['in'])\n",
    "                entities_mixed[j][k] += temp\n",
    "        namegroup = findsimilarname(entities_mixed)\n",
    "        samein = []\n",
    "        for obj in namegroup:\n",
    "            samein.append([])\n",
    "            for obj2 in obj:\n",
    "                if obj2[5] == 'in':\n",
    "                    for obj3 in obj:\n",
    "                        for obj4 in entities_extended:\n",
    "                            mt = 0\n",
    "                            for l in range(4):\n",
    "                                if obj4[l] == obj3[l]:\n",
    "                                    mt += 1\n",
    "                            if mt == 4:\n",
    "                                samein[-1].append(obj4)\n",
    "        while [] in samein:\n",
    "            samein.remove([])\n",
    "        for j in range(3):\n",
    "            for k in range(len(entities_out[i][j])):\n",
    "                for obj in entities_out[i][j][k]:\n",
    "                    found = 0\n",
    "                    found_else = 0\n",
    "                    for obj2 in namegroup:\n",
    "                        for obj3 in obj2:\n",
    "                            match = 0\n",
    "                            for l in range(4):\n",
    "                                if obj[l] == obj3[l]:\n",
    "                                    match += 1\n",
    "                            if match == 4:\n",
    "                                for obj4 in obj2:\n",
    "                                    if obj4[5] == 'in':\n",
    "                                        found = 1\n",
    "                    rest = len(samein)-found\n",
    "                    if rest == 0:\n",
    "                        found_else = 0\n",
    "                    else:\n",
    "                        found_else = 1\n",
    "                    temp_qf = [num_qm,num_quote,found,found_else,distance,length]\n",
    "                    qf[-1].append(temp_qf)\n",
    "    return qf\n",
    "qf = quotefeature(instance,entities_out,entities_in)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dst_target = []\n",
    "feature_target = []\n",
    "order_target = []\n",
    "for i in range(len(entities_target_out)):\n",
    "    dst_target.append(distance(instance_target,entities_target_out,i,keyposition_target))\n",
    "    feature_target.append(dst_target[i][0][:])\n",
    "    order_target.append(feature_target[i][:])\n",
    "    temp = sorted(range(len(order_target[i])), key=lambda k: order_target[i][k])\n",
    "    n = 0\n",
    "    for obj in temp:\n",
    "        order_target[i][obj].append(n)\n",
    "        n += 1\n",
    "dp_target = []\n",
    "for i in range(len(entities_target_out)):\n",
    "    dp_target.append(detailparagraph(instance_target,i,entities_target_out,keyposition_target))\n",
    "nb_target = []\n",
    "for i in range(len(instance_target)):            \n",
    "    nb_target.append(neighbourhood(instance_target,i,entities_target_out,result_target))\n",
    "ce_target = []\n",
    "ce_target = countentities(instance_target,entities_target_out)\n",
    "nocc_target = []\n",
    "nocc_target = nameoccurrence(entities_target_out,keyposition_target)\n",
    "tk_target = []\n",
    "tk_target = tokenize(instance_target,entities_target_out,result_target)\n",
    "qf_target = []\n",
    "qf_target = quotefeature(instance_target,entities_target_out,entities_target_in)\n",
    "for i in range(len(feature_target)):\n",
    "    for j in range(len(feature_target[i])):\n",
    "        feature_target[i][j] += dst_target[i][1][j]\n",
    "        feature_target[i][j] += dp_target[i]\n",
    "        #feature_target[i][j] += nb_target[i][j]\n",
    "        feature_target[i][j] += ce_target[i][j]\n",
    "        feature_target[i][j] += nocc_target[i][j]\n",
    "        feature_target[i][j] += tk_target[i][j]\n",
    "        feature_target[i][j] += qf_target[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(feature)):\n",
    "    for j in range(len(feature[i])):\n",
    "        feature[i][j] += dst[i][1][j]\n",
    "        feature[i][j] += dp[i]\n",
    "        #feature[i][j] += nb[i][j]\n",
    "        feature[i][j] += ce[i][j]\n",
    "        feature[i][j] += nocc[i][j]\n",
    "        feature[i][j] += tk[i][j]\n",
    "        feature[i][j] += qf[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "position_fact = []\n",
    "for i in range(len(entities_out)):\n",
    "    pos = 0\n",
    "    if fact[i][0] == 0:\n",
    "        pos = fact[i][2]\n",
    "        position_fact.append(pos)\n",
    "    elif fact[i][0] == 1:\n",
    "        pos += len(entities_out[i][0][0])\n",
    "        for j in range(fact[i][1]):\n",
    "            pos += len(entities_out[i][1][j])\n",
    "        pos += fact[i][2]\n",
    "        position_fact.append(pos)\n",
    "    elif fact[i][0] == 2:\n",
    "        pos += len(entities_out[i][0][0])\n",
    "        for j in range(len(entities_out[i][1])):\n",
    "            pos += len(entities_out[i][1][j])\n",
    "        for j in range(fact[i][1]):\n",
    "            pos += len(entities_out[i][2][j])\n",
    "        pos += fact[i][2]\n",
    "        position_fact.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "for obj in feature:\n",
    "    X += obj\n",
    "Y = []\n",
    "for i in range(len(feature)):\n",
    "    temp = []\n",
    "    for j in range(len(feature[i])):\n",
    "        if j == position_fact[i]:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    Y += temp\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findname(index,num_instance):\n",
    "    counter = 0\n",
    "    temp_i = 0\n",
    "    temp_j = 0\n",
    "    temp_counter = 0\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities_target_out[num_instance][i])):\n",
    "            temp_counter = counter\n",
    "            counter += len(entities_target_out[num_instance][i][j])\n",
    "            temp_j = j\n",
    "            temp_i = i\n",
    "            if index-counter < 0:\n",
    "                offset = index-temp_counter\n",
    "                return temp_i,temp_j,offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 7, ['Robb', 221, 528, 2776, 'PERSON'], 0.9999999746632969]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 5, ['Ser Brynden', 317, 751, 3925, 'PERSON'], 0.9999424147266055]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['son', 31, 748, 3838, 'NOUN'], 0.9999878202293577]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['uncle', 108, 668, 3435, 'NOUN'], 0.9999751797420163]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Galbart Glover', 45, 643, 3340, 'PERSON'], 0.9994326626425336]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Ser Brynden', 17, 522, 2706, 'PERSON'], 0.9999869160038954]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Robb', 9, 490, 2565, 'PERSON'], 0.9999947492528924]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['Robb', 9, 406, 2139, 'PERSON'], 0.355760667149837]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 9, ['Robb', 402, 450, 2387, 'PERSON'], 0.5874053312171487]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 4, ['lords bannermen', 348, 495, 2647, 'NOUN'], 0.0087887118198594]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Robb', 31, 313, 1715, 'PERSON'], 0.9988199848705643]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 30, ['Catelyn', 983, 486, 2667, 'PERSON'], 8.422606329580204e-12]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Hal Mollen', 29, 530, 2878, 'PERSON'], 0.007132041413734532]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Ser Brynden', 80, 515, 2833, 'PERSON'], 0.20433167681343492]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 2, ['Ned', 61, 541, 2983, 'PERSON'], 0.0068546422052043286]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 5, ['Robb', 378, 405, 2262, 'PERSON'], 0.9999991094060179]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['men', 39, 424, 2300, 'NOUN'], 0.9990469959117301]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 3, ['Robb', 265, 646, 3498, 'PERSON'], 0.9999501893551448]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 4, ['Catelyn', 318, 646, 3503, 'PERSON'], 0.9951300706839605]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 4, ['Hal', 224, 684, 3709, 'PERSON'], 0.9999697621237934]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Lannister', 2, 624, 3359, 'PERSON'], 0.9999793077531233]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Catelyn', 12, 625, 3329, 'PERSON'], 0.9999804432324174]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Theon Greyjoy', 20, 557, 2990, 'PERSON'], 0.9999975604587149]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Catelyn', 12, 474, 2534, 'PERSON'], 0.9999999769340493]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Theon Greyjoy', 20, 466, 2502, 'PERSON'], 0.9999999691127646]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['son', 12, 287, 1531, 'NOUN'], 0.9999999986603427]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jaime Lannister', 16, 287, 1545, 'PERSON'], 0.9999999994685993]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Catelyn', 40, 218, 1194, 'PERSON'], 0.999999999730079]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Robb', 31, 199, 1089, 'PERSON'], 0.999999997714724]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Greatjon', 22, 181, 971, 'PERSON'], 0.9999999997619966]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Catelyn', 44, 149, 840, 'PERSON'], 0.9999999997369002]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Robb', 23, 153, 896, 'PERSON'], 0.9999999994976463]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Galbart Glover', 26, 138, 806, 'PERSON'], 0.9999999998192379]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Robb', 23, 140, 822, 'PERSON'], 0.9999999998564704]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Glover', 47, 147, 863, 'PERSON'], 0.9999999998109956]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Robb', 124, 173, 988, 'PERSON'], 0.999999999945743]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Catelyn', 58, 193, 1075, 'PERSON'], 0.999999999938268]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Theon Greyjoy', 28, 254, 1388, 'PERSON'], 0.999999999726839]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Catelyn', 20, 314, 1732, 'PERSON'], 0.999999998367656]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Greyjoy', 8, 302, 1706, 'PERSON'], 0.9999999996400675]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['Greyjoy', 8, 299, 1647, 'PERSON'], 0.9999999439685625]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Robb', 2, 294, 1622, 'PERSON'], 0.9999999917711193]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon Snow', 31, 220, 1218, 'PERSON'], 0.9999999993785309]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Sam', 20, 268, 1462, 'PERSON'], 0.9999997873002484]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Jon', 33, 161, 1027, 'PERSON'], 0.9999999901153842]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Sam', 20, 163, 878, 'PERSON'], 0.9999999861165066]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 33, 170, 896, 'PERSON'], 0.9999999712832737]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 5, 0, ['Sam', 20, 148, 789, 'PERSON'], 0.9999970872285717]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 7, ['Jon', 538, 340, 1736, 'PERSON'], 0.9999452474699091]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 8, ['Jon', 389, 937, 4794, 'PERSON'], 0.0769878092187365]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 3, ['whores', 319, 833, 4297, 'NOUN'], 0.27607681199532924]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 1, ['prey', 136, 745, 3941, 'NOUN'], 0.9317704763896508]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 12, 716, 3806, 'PERSON'], 0.9999999808458429]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['sentinels', 67, 713, 3778, 'NOUN'], 0.9999608019475624]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 7, 7, ['farmers', 270, 632, 3334, 'NOUN'], 0.9994503009246337]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 7, 0, ['kingsroad', 94, 577, 3048, 'NOUN'], 0.9997527509516271]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Grenn', 17, 553, 2875, 'PERSON'], 0.9999652443815145]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Pyp', 32, 508, 2643, 'PERSON'], 0.9999959958188659]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 17, 434, 2246, 'PERSON'], 0.9999997157729701]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 32, 368, 1888, 'PERSON'], 0.999999993444078]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['Pyp', 32, 283, 1457, 'PERSON'], 0.999999934644196]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Toad', 116, 229, 1172, 'PERSON'], 0.9999999847686071]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Halder', 28, 176, 897, 'PERSON'], 0.9999999994579412]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 0, ['Grenn', 15, 165, 839, 'PERSON'], 0.9999999844202422]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 7, 0, ['Halder', 28, 135, 658, 'PERSON'], 0.9999999865249266]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 15, 155, 781, 'PERSON'], 0.9999999995203268]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['Grenn', 15, 131, 669, 'PERSON'], 0.999999986613716]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 2, ['Pyp', 143, 170, 907, 'NOUN'], 0.9999999939216195]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Halder', 18, 173, 917, 'PERSON'], 0.9999999998122178]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['Halder', 18, 168, 893, 'PERSON'], 0.9999999754810748]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 2, ['Jon', 161, 177, 967, 'PERSON'], 0.9999999999337206]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 3, ['Halder', 208, 174, 942, 'PERSON'], 0.9999999836743144]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 13, 172, 936, 'PERSON'], 0.9999999999541274]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 9, 201, 1073, 'PERSON'], 0.9999999994049347]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 13, 202, 1078, 'PERSON'], 0.99999999943131]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 2, 209, 1108, 'PERSON'], 0.9999999979563938]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Halder', 23, 164, 937, 'PERSON'], 0.99999999849868]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 30, 160, 849, 'PERSON'], 0.9999999998679812]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 47, 154, 820, 'PERSON'], 0.9999999996872759]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 7, 0, ['Jon', 30, 139, 733, 'PERSON'], 0.999999998139117]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 29, 122, 639, 'PERSON'], 0.9999999999604654]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Toad', 59, 132, 690, 'PERSON'], 0.9999999986361559]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 17, 119, 629, 'PERSON'], 0.9999999999563443]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 14, 131, 680, 'PERSON'], 0.9999999999365343]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 18, 146, 767, 'PERSON'], 0.9999999995595772]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 35, 133, 725, 'PERSON'], 0.9999999997716316]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 15, 123, 751, 'PERSON'], 0.999999999746791]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 24, 141, 751, 'PERSON'], 0.9999999986561363]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Grenn', 37, 155, 818, 'PERSON'], 0.9999999994134612]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 0, 0, ['Halder', 36, 196, 1008, 'PERSON'], 0.9999999768309922]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Halder', 36, 189, 967, 'PERSON'], 0.9999999996979625]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Toad', 36, 166, 863, 'PERSON'], 0.9999999953180918]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 1, ['Pyp', 59, 161, 842, 'PERSON'], 0.9999999941726401]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 15, 173, 915, 'PERSON'], 0.9999999995363282]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Matthar', 2, 190, 991, 'PERSON'], 0.999999993743586]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Halder', 23, 202, 1063, 'PERSON'], 0.9999999980695691]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 44, 194, 1029, 'PERSON'], 0.9999999787619915]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 2, 203, 1069, 'PERSON'], 0.9999999827973626]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Halder', 98, 202, 1091, 'PERSON'], 0.9999999528485273]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 1, ['Jon', 83, 212, 1130, 'PERSON'], 0.9999999974896525]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Pyp', 23, 227, 1205, 'PERSON'], 0.999999992478223]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 1, 1, ['boy', 131, 434, 2306, 'NOUN'], 0.9999469501030459]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 13, 387, 2044, 'PERSON'], 0.9999999997918394]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 1, ['boy', 131, 405, 2142, 'NOUN'], 0.999999303650001]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 8, ['Jon', 449, 481, 2541, 'PERSON'], 0.9999999994126938]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 1, ['Jon', 55, 532, 2816, 'PERSON'], 0.9999999794927705]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 16, 518, 2761, 'PERSON'], 0.9999999937788289]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 8, 0, ['Jon', 16, 496, 2650, 'PERSON'], 0.9999997747019022]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 2, 464, 2428, 'PERSON'], 0.9999998385634244]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Mormont', 32, 343, 1813, 'PERSON'], 0.9999999846258448]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 0, 0, ['Jon', 32, 374, 1956, 'PERSON'], 0.9999999608926665]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 32, 368, 1920, 'PERSON'], 0.9999999999481304]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Mormont', 33, 333, 1741, 'PERSON'], 0.9999999962251707]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[1, 7, 0, ['Jon', 32, 222, 1152, 'PERSON'], 0.9999999961189587]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Mormont', 61, 199, 1028, 'PERSON'], 0.9999999998812257]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 8, 223, 1134, 'PERSON'], 0.9999999992899689]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 0, 0, ['Jon', 2, 216, 1097, 'PERSON'], 0.9999999445205974]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 2, 181, 922, 'PERSON'], 0.9999999984623855]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 0, 0, ['Mormont', 22, 215, 1095, 'PERSON'], 0.9999999790807694]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Mormont', 22, 206, 1035, 'PERSON'], 0.9999999996570352]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Mormont', 18, 214, 1089, 'PERSON'], 0.9999999976351148]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 0, 0, ['Mormont', 10, 229, 1174, 'PERSON'], 0.9999994314329594]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Mormont', 10, 218, 1114, 'PERSON'], 0.9999999998236433]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 1, ['Mormont', 423, 363, 1842, 'PERSON'], 0.99999999874683]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 19, 415, 2085, 'PERSON'], 0.9999999414635016]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Lord Commander Mormont', 15, 416, 2078, 'PERSON'], 0.9999999996597921]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[2, 0, 0, ['Jon', 8, 565, 2911, 'PERSON'], 0.9999995138847781]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['Jon', 8, 495, 2562, 'PERSON'], 0.9999999952523524]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n",
      "[0, 0, 0, ['man', 73, 514, 2659, 'NOUN'], 0.999999991488238]\n",
      "continue? : 1\n",
      "#####################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(len(feature_target)):\n",
    "    temp = []\n",
    "    for i in range(len(feature_target[t])):\n",
    "        nc,nl,n = findname(i,t)\n",
    "        temp.append([nc,nl,n,entities_target_out[t][nc][nl][n],clf.predict_proba([feature_target[t][i]])[0][1].item()])\n",
    "    print sorted(temp,key=itemgetter(4))[-1]\n",
    "    if input(\"continue? : \") == 1:\n",
    "        print '#####################\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature [1 distance in words from the quote, \n",
    "#         1 the ordinal position, \n",
    "#         6 numbers of ',', '.', '\\n', '!', '?', '\"' between name and quote,\n",
    "#         45 Number of names, quotes and words in each one of 15 paragraphs\n",
    "#         10 words of the neighbourhood(unused)\n",
    "#         1 number of entities between name and quote\n",
    "#         1 total number of appearance of this entity\n",
    "#         15 numbers of appearance of this entity in each paragraph\n",
    "#         6 numbers of tokens 3 each side of the target]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
