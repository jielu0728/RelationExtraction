{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import jsonrpclib\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk.data\n",
    "import re\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "import ast\n",
    "from simplejson import loads\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "r = re.compile(r'[{}]'.format(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name appears in the paragraph before\n",
    "\n",
    "\n",
    "2 alternant lines \n",
    "\n",
    "if the name appears in the previous or following utterance with a pattern such as ',name,'\n",
    "\n",
    "the object of the speech verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('annotation_speaker')\n",
    "n = 0\n",
    "instance = []\n",
    "temp = []\n",
    "speaker = []\n",
    "p = 0\n",
    "keyposition = []   #the linenum of the paragraph to be treated\n",
    "for line in f:\n",
    "    if n <= 29560:\n",
    "        if '###############' in line:\n",
    "            keyposition.append(p)\n",
    "            current = [temp.pop()]\n",
    "            previous = temp[:]\n",
    "            temp = []\n",
    "            continue\n",
    "        if 'speaker:\t' in line:\n",
    "            speaker.append(line.split('speaker:\t')[1].split('\\r\\n')[0])\n",
    "            instance.append([current,previous,temp])\n",
    "            temp = []\n",
    "            p = 0\n",
    "            continue\n",
    "        if line.replace(' ','') != '\\r\\n':\n",
    "            temp.append(line.replace('\\r\\n',''))\n",
    "            p = p+1\n",
    "    n = n+1\n",
    "    \n",
    "\n",
    "f.close()\n",
    "#instance [0,1,2]:\n",
    "#0 : current paragraph (including the object utterance)\n",
    "#1 : 9 previous paragraphs\n",
    "#2 : 5 following paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = './result_parser.json'\n",
    "\n",
    "def treatresultparser(f):\n",
    "    f = open(f)\n",
    "    result = []\n",
    "    for line in f:\n",
    "        if '#######seperator instance' in line:\n",
    "            result.append([])\n",
    "        elif '#######seperator cases' in line:\n",
    "            result[-1].append([])\n",
    "        elif '########seperator paragraphs' in line:\n",
    "            result[-1][-1].append([])\n",
    "        elif '{' in line:\n",
    "            parsetree = []\n",
    "            words = []\n",
    "            dependency = []\n",
    "            pos = [match.end() for match in re.finditer(re.escape('{\"parsetree\": \"'), line)]\n",
    "            pos2 = [match.start() for match in re.finditer(re.escape(')\", \"text\"'), line)]\n",
    "            pos3 = [match.end() for match in re.finditer(re.escape('\"words\": [['), line)]\n",
    "            pos4 = [match.start() for match in re.finditer(re.escape('], \"indexeddependencies\":'), line)]\n",
    "            pos5 = [match.end() for match in re.finditer(re.escape('\"indexeddependencies\": ['), line)]\n",
    "            pos6 = [match.start() for match in re.finditer(re.escape('}, {\"parsetree\":'), line)]\n",
    "            pos6.append(line.index(']}]}'))\n",
    "            for i in range(len(pos)):\n",
    "                parsetree.append(line[pos[i]:pos2[i]+1])\n",
    "                words.append(line[pos3[i]:pos4[i]+1])\n",
    "            for i in range(len(pos5)):\n",
    "                dependency.append(line[pos5[i]:pos6[i]])\n",
    "            result[-1][-1][-1].append(parsetree)\n",
    "            result[-1][-1][-1].append(words)\n",
    "            for i in range(len(dependency)):\n",
    "                if len(dependency[i]) > 2:\n",
    "                    if dependency[i][-2:] == ']]' :\n",
    "                        dependency[i] = dependency[i][:-1]\n",
    "                if dependency[i] == ']':\n",
    "                    dependency[i] = '[]'\n",
    "            result[-1][-1][-1].append(dependency)\n",
    "    f.close()\n",
    "    return result\n",
    "\n",
    "result = treatresultparser(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treatresultparser_new(f):\n",
    "    f = open(f)\n",
    "    result = []\n",
    "    for line in f:\n",
    "        if '#######seperator' in line:\n",
    "            result.append([])\n",
    "        elif '{' in line:\n",
    "            parsetree = []\n",
    "            words = []\n",
    "            dependency = []\n",
    "            pos = [match.end() for match in re.finditer(re.escape('{\"parsetree\": \"'), line)]\n",
    "            pos2 = [match.start() for match in re.finditer(re.escape(')\", \"text\"'), line)]\n",
    "            pos3 = [match.end() for match in re.finditer(re.escape('\"words\": [['), line)]\n",
    "            pos4 = [match.start() for match in re.finditer(re.escape('], \"indexeddependencies\":'), line)]\n",
    "            pos5 = [match.end() for match in re.finditer(re.escape('\"indexeddependencies\": ['), line)]\n",
    "            pos6 = [match.start() for match in re.finditer(re.escape('}, {\"parsetree\":'), line)]\n",
    "            pos6.append(line.index(']}]}'))\n",
    "            for i in range(len(pos)):\n",
    "                parsetree.append(line[pos[i]:pos2[i]+1])\n",
    "                words.append(line[pos3[i]:pos4[i]+1])\n",
    "            for i in range(len(pos5)):\n",
    "                dependency.append(line[pos5[i]:pos6[i]])\n",
    "            result[-1].append(parsetree)\n",
    "            result[-1].append(words)\n",
    "            for i in range(len(dependency)):\n",
    "                if len(dependency[i]) > 2:\n",
    "                    if dependency[i][-2:] == ']]' :\n",
    "                        dependency[i] = dependency[i][:-1]\n",
    "                if dependency[i] == ']':\n",
    "                    dependency[i] = '[]'\n",
    "            result[-1].append(dependency)\n",
    "    f.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 7, 6, 16, 83, 9, 25, 3, 42, 53, 82, 62, 143, 109, 35]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countwordperline(instance,num_instance,keyposition):\n",
    "    numlist = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(instance[num_instance][i])):\n",
    "            new_strs = r.sub(' ',instance[num_instance][i][j])\n",
    "            num = len(new_strs.split())\n",
    "            numlist.append(num)\n",
    "    numlist.insert(keyposition[num_instance]-1, numlist.pop(0))\n",
    "    return numlist\n",
    "            \n",
    "countwordperline(instance,62,keyposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertoffset(numlist,linenum,offset,sentence):\n",
    "    temp = sentence[0:offset]\n",
    "    new_strs = r.sub(' ',temp)\n",
    "    num = len(new_strs.split())\n",
    "    pos = 0\n",
    "    for i in range(linenum-1):\n",
    "        pos = pos + numlist[i]\n",
    "    return pos+num+1\n",
    "\n",
    "convertoffset(countwordperline(instance,0,keyposition),keyposition[0],33,instance[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# no use, we can get the index from the loop number\n",
    "def convertposition(numlist,linenum,position): \n",
    "    temp = position\n",
    "    i = 0 \n",
    "    while temp > 0:\n",
    "        temp = temp - numlist[i]\n",
    "        i = i+1\n",
    "    i = i-1\n",
    "    if i == linenum-1:\n",
    "        return 0,0\n",
    "    elif i>linenum-1:\n",
    "        return 2,i-linenum\n",
    "    else:\n",
    "        return 1,i\n",
    "\n",
    "convertposition(countwordperline(instance,0),keyposition[0],669)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def infodependency(name,offset,ni,nc,np):\n",
    "    new_strs = r.sub(' ',instance[ni][nc][np])\n",
    "    num = len(new_strs.split())\n",
    "    part = tokenizer.tokenize(instance[ni][nc][np])\n",
    "    length = []\n",
    "    t = len(part)-1\n",
    "    i = 0\n",
    "    while i <= t:\n",
    "        if len(part[i]) > 3:\n",
    "            if part[i][-3:] == '...' and i < t:\n",
    "                part[i] = part[i] +' '+ part[i+1]\n",
    "                part.pop(i+1)\n",
    "                i -= 1\n",
    "                t -= 1\n",
    "                continue\n",
    "            if part[i][:3] == 'And':\n",
    "                part[i-1] = part[i-1] +' '+ part[i]\n",
    "                part.pop(i)\n",
    "                i -= 1\n",
    "                t -= 1\n",
    "                continue\n",
    "        i += 1\n",
    "    for i in range(len(part)):\n",
    "        if i == 0:\n",
    "            length.append(len(part[i]))\n",
    "        elif i > 0:\n",
    "            part[i] = ' '+part[i]\n",
    "            length.append(len(part[i]))\n",
    "\n",
    "    s = 0\n",
    "    linenum = 0\n",
    "    offset_new = 0\n",
    "    for i in range(len(length)):\n",
    "        s += length[i]\n",
    "        if offset - s < 0:\n",
    "            linenum = i\n",
    "            offset_new = offset - length[i-1]\n",
    "            break\n",
    "    key = name+'-'+str(len(re.findall(r\"[\\w]+|[^\\s\\w]\", part[linenum][:offset_new]))+1)\n",
    "    temp = ast.literal_eval(result[ni][nc][np][2][linenum])\n",
    "    head = []\n",
    "    for obj in temp:\n",
    "        if key in obj:\n",
    "            if obj[0] == 'det' or obj[0] == 'amod':\n",
    "                head.append(obj[2].split('-'))\n",
    "    head.sort(key=itemgetter(1))\n",
    "    complete = ''\n",
    "    for obj in head:\n",
    "        complete += obj[0]+' '\n",
    "    complete += name\n",
    "    return complete\n",
    "    \n",
    "infodependency('lady',49,116,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this offset global contains newline\n",
    "def convertoffsetglobal(offsetlocal,ni,nc,nl,instance):\n",
    "    if nc == 0:\n",
    "        for k in range(len(instance[ni][1])):\n",
    "            offsetlocal += len(instance[ni][1][k]) +1\n",
    "    elif nc == 1:\n",
    "        for k in range(nl):\n",
    "            offsetlocal += len(instance[ni][1][k]) +1\n",
    "    elif nc == 2:\n",
    "        for k in range(len(instance[ni][1])):\n",
    "            offsetlocal += len(instance[ni][1][k]) +1\n",
    "        for k in range(nl):\n",
    "            offsetlocal += len(instance[ni][2][k]) +1\n",
    "        offsetlocal += len(instance[ni][0][0]) +1\n",
    "    return offsetlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ner(instance,result,keyposition):\n",
    "    entities = []\n",
    "    n = 0\n",
    "    for obj in instance:\n",
    "        numlist = countwordperline(instance,n,keyposition)\n",
    "        current = [[]]\n",
    "        temp = []\n",
    "        for z in range(len(result[n][0][0][0])):\n",
    "            temp = temp + result[n][0][0][0][z].split('] [')\n",
    "        for frag in temp:\n",
    "            if 'PartOfSpeech=NNP' in frag or 'NamedEntityTag=PERSON' in frag:\n",
    "                text = frag.split('Text=')[1].split(' ')[0]\n",
    "                position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                current[0].append([text,position,pos,offset,'PERSON'])\n",
    "            elif 'PartOfSpeech=N' in frag:\n",
    "                text = frag.split('Text=')[1].split(' ')[0]\n",
    "                position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                try:\n",
    "                    pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                except IndexError:\n",
    "                    print \n",
    "                    ccccccc\n",
    "                lex = []\n",
    "                for synset in wn.synsets(text):\n",
    "                    lex.append(synset.lexname())\n",
    "                lenlex = 0\n",
    "                for c in lex:\n",
    "                    if 'noun' in c:\n",
    "                        lenlex += 1\n",
    "                if lex == []:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "                elif lenlex == 0:\n",
    "                    continue\n",
    "                elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "        temp = []\n",
    "        for z in range(len(result[n][0][0][1])):\n",
    "            temp = temp + result[n][0][0][1][z].split('] [')\n",
    "        for frag in temp:\n",
    "            part = frag.split('\"')\n",
    "            if part[part.index('PartOfSpeech')+2] == 'NNP' or part[part.index('NamedEntityTag')+2] == 'PERSON':\n",
    "                text = frag.split('\"')[1]\n",
    "                position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                current[0].append([text,position,pos,offset,'PERSON'])\n",
    "            elif 'N' in part[part.index('PartOfSpeech')+2]:\n",
    "                text = frag.split('\"')[1]\n",
    "                position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                offset = convertoffsetglobal(position,n,0,0,instance)\n",
    "                pos = convertoffset(numlist,keyposition[n],position,obj[0][0])\n",
    "                lex = []\n",
    "                for synset in wn.synsets(text):\n",
    "                    lex.append(synset.lexname())\n",
    "                lenlex = 0\n",
    "                for c in lex:\n",
    "                    if 'noun' in c:\n",
    "                        lenlex += 1\n",
    "                if lex == []:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "                elif lenlex == 0:\n",
    "                    continue\n",
    "                elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                    current[0].append([text,position,pos,offset,'NOUN'])\n",
    "        previous = []\n",
    "        for i in range(len(obj[1])):\n",
    "            previous.append([])\n",
    "            temp = []\n",
    "            for x in range(len(result[n][1][i][0])):\n",
    "                try:\n",
    "                    temp = temp + result[n][1][i][0][x].split('] [')\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for frag in temp:\n",
    "                if 'PartOfSpeech=NNP' in frag or 'NamedEntityTag=PERSON' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    previous[i].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'PartOfSpeech=N' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "            temp = []\n",
    "            for x in range(len(result[n][1][i][1])):\n",
    "                temp = temp + result[n][1][i][1][x].split('] [')\n",
    "            for frag in temp:\n",
    "                part = frag.split('\"')\n",
    "                if part[part.index('PartOfSpeech')+2] == 'NNP' or part[part.index('NamedEntityTag')+2] == 'PERSON':\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    previous[i].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'N' in part[part.index('PartOfSpeech')+2]:\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,1,i,instance)\n",
    "                    pos = convertoffset(numlist,i+1,position,obj[1][i])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                        previous[i].append([text,position,pos,offset,'NOUN'])\n",
    "        following = []\n",
    "        for j in range(len(obj[2])):\n",
    "            following.append([])\n",
    "            temp = []\n",
    "            for y in range(len(result[n][2][j][0])):\n",
    "                try:\n",
    "                    temp = temp + result[n][2][j][0][y].split('] [')\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for frag in temp:\n",
    "                if 'PartOfSpeech=NNP' in frag or 'NamedEntityTag=PERSON' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    following[j].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'PartOfSpeech=N' in frag:\n",
    "                    text = frag.split('Text=')[1].split(' ')[0]\n",
    "                    position = int(frag.split('CharacterOffsetBegin=')[1].split(' ')[0])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(lenlex) >= 0.4:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "            temp = []\n",
    "            for y in range(len(result[n][2][j][1])):\n",
    "                temp = temp + result[n][2][j][1][y].split('] [')\n",
    "            for frag in temp:\n",
    "                part = frag.split('\"')\n",
    "                if part[part.index('PartOfSpeech')+2] == 'NNP' or part[part.index('NamedEntityTag')+2] == 'PERSON':\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    following[j].append([text,position,pos,offset,'PERSON'])\n",
    "                elif 'N' in part[part.index('PartOfSpeech')+2]:\n",
    "                    text = frag.split('\"')[1]\n",
    "                    position = int(part[part.index('CharacterOffsetBegin')+2])\n",
    "                    offset = convertoffsetglobal(position,n,2,j,instance)\n",
    "                    pos = convertoffset(numlist,keyposition[n]+j+1,position,obj[2][j])\n",
    "                    lex = []\n",
    "                    for synset in wn.synsets(text):\n",
    "                        lex.append(synset.lexname())\n",
    "                    lenlex = 0\n",
    "                    for c in lex:\n",
    "                        if 'noun' in c:\n",
    "                            lenlex += 1\n",
    "                    if lex == []:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "                    elif lenlex == 0:\n",
    "                        continue\n",
    "                    elif float(lex.count('noun.person'))/float(len(lex)) >= 0.4:\n",
    "                        following[j].append([text,position,pos,offset,'NOUN'])\n",
    "        entities.append([current,previous,following])\n",
    "        n = n +1\n",
    "    return entities\n",
    "\n",
    "entities = ner(instance,result,keyposition)\n",
    "for i in range(len(entities)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities[i][j])):\n",
    "            entities[i][j][k].sort(key=itemgetter(2))\n",
    "entities_new = entities[:]\n",
    "\n",
    "#entities [name, offset in the paragraph, word position in the instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine contiguous name entities and separate those within and outside quotation marks\n",
    "def contiguous(instance,namelist,ni,cls,nl):\n",
    "    i = 0\n",
    "    t = len(namelist)-1\n",
    "    newlist = {}\n",
    "    newlist['in'] = []\n",
    "    newlist['out'] = []\n",
    "    while i < t:\n",
    "        if namelist[i+1][1]-namelist[i][1] == len(namelist[i][0])+1 and instance[ni][cls][nl][namelist[i][1]+len(namelist[i][0])] == ' ' and namelist[i+1][4] == namelist[i][4]:\n",
    "            namelist[i][0] = namelist[i][0]+' '+namelist[i+1][0]\n",
    "            namelist.pop(i+1)\n",
    "            t = t - 1\n",
    "            i = i - 1\n",
    "        i = i + 1\n",
    "    for obj in namelist:\n",
    "        if instance[ni][cls][nl][0:obj[1]].count('\"')%2 == 1:\n",
    "            newlist['in'].append(obj)\n",
    "        else:\n",
    "            newlist['out'].append(obj)\n",
    "    return newlist\n",
    "\n",
    "for i in range(len(entities)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(instance[i][j])):\n",
    "            entities_new[i][j][k] = contiguous(instance,entities[i][j][k],i,j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findentities_out(entities_new):\n",
    "    entities_out = []\n",
    "    entities_in = []\n",
    "    for i in range(len(entities_new)):\n",
    "        entities_out.append([])\n",
    "        entities_in.append([])\n",
    "        for j in range(3):\n",
    "            entities_out[i].append([])\n",
    "            entities_in[i].append([])\n",
    "            for k in range(len(entities_new[i][j])):\n",
    "                if entities_new[i][j][k] == {}:\n",
    "                    entities_out[i][j].append([])\n",
    "                    entities_in[i][j].append([])\n",
    "                else:\n",
    "                    entities_out[i][j].append([])\n",
    "                    entities_out[i][j][k] = entities_new[i][j][k]['out']\n",
    "                    entities_in[i][j].append([])\n",
    "                    entities_in[i][j][k] = entities_new[i][j][k]['in']\n",
    "    return entities_out,entities_in\n",
    "\n",
    "entities_out,entities_in = findentities_out(entities_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####only for noting who is the speaker from the candidates\n",
    "i = 0\n",
    "#print instance[i]\n",
    "fact = []\n",
    "for i in range(len(instance)):\n",
    "    print instance[i][0][0]\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities_out[i][j])):\n",
    "            for l in range(len(entities_out[i][j][k])):\n",
    "                print entities_out[i][j][k][l], str(j), str(k), str(l) + '\\n'\n",
    "    fact.append(input(\"which : \"))\n",
    "#print result[i][0][0][2][1]\n",
    "print fact\n",
    "####maybe I should add those adj and det and see if it improve the result######\n",
    "#####The Tattered Prince#####\n",
    "#####the High Septon######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('./fact')\n",
    "fact = []\n",
    "for line in f:\n",
    "    fact = ast.literal_eval(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###is there any quote in the paragraph which this person shows\n",
    "###how many quotes between this person and the target quote\n",
    "###all the index of paragraphs in which this person shows\n",
    "###the last puctuation marks of the last quote and is there a close quotation mark\n",
    "###whether this is a name entity or a personal noun\n",
    "###whether this person is a object of a speech verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "notfound = []\n",
    "for i in range(len(entities_out)):\n",
    "    found = 0\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities_out[i][j])):\n",
    "            for obj in entities_out[i][j][k]:\n",
    "                if speaker[i] == obj[0]:\n",
    "                    found = 1\n",
    "    if found == 0:\n",
    "        notfound.append(i)\n",
    "print notfound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countpunctuation(sentence,l):\n",
    "    punctuation = [',', '.', '\\n', '!', '?', '\"']\n",
    "    for i in range(len(punctuation)):\n",
    "        l[i] = l[i] + sentence.count(punctuation[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to extract the distance between the candidat and the quote, with counting the punctuations between them\n",
    "def distance(instance,entities,num_instance,keyposition):\n",
    "    sentence = instance[num_instance][0][0]\n",
    "    numlist = countwordperline(instance,num_instance,keyposition)\n",
    "    if sentence.count('\"') == 1:\n",
    "        offset = [sentence.index('\"')]\n",
    "        position = [convertoffset(countwordperline(instance,num_instance,keyposition),keyposition[num_instance],offset[0],instance[num_instance][0][0])]\n",
    "    else:\n",
    "        offset = []\n",
    "        p = 0\n",
    "        for c in sentence:\n",
    "            if c == '\"':\n",
    "                offset.append(p)\n",
    "            p = p+1\n",
    "        position = []\n",
    "        for i in range(2):\n",
    "            position.append(convertoffset(countwordperline(instance,num_instance,keyposition),keyposition[num_instance],offset[i],instance[num_instance][0][0]))\n",
    "    dist = []\n",
    "    cp = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities[num_instance][i])):\n",
    "            for k in range(len(entities[num_instance][i][j])):\n",
    "                cp.append([0,0,0,0,0,0])\n",
    "                pos = entities[num_instance][i][j][k][2]\n",
    "                ofs = entities[num_instance][i][j][k][1]\n",
    "                if len(position) == 1:\n",
    "                    dist.append([abs(pos-position[0])])\n",
    "                else:\n",
    "                    if pos <= position[0]:\n",
    "                        dist.append([position[0]-pos])\n",
    "                    elif pos >= position[1]:\n",
    "                        dist.append([pos-position[1]])\n",
    "                stc = instance[num_instance][i][j]\n",
    "                if len(offset)>1:\n",
    "                    if i == 1:\n",
    "                        countpunctuation(stc[ofs:],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                        for s in range(j+1,keyposition[num_instance]-1):\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                        countpunctuation(sentence[:offset[0]],cp[-1])\n",
    "                    elif (i == 0 and ofs < offset[0]):\n",
    "                        countpunctuation(sentence[ofs:offset[0]],cp[-1])\n",
    "                    elif i == 2:\n",
    "                        countpunctuation(stc[:ofs],cp[-1])\n",
    "                        for s in range(j):\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                        countpunctuation(sentence[offset[1]+1:],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                    else:\n",
    "                        countpunctuation(sentence[offset[1]+1:ofs+1],cp[-1])\n",
    "                else:\n",
    "                    if i == 1:    \n",
    "                        countpunctuation(stc[ofs:],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                        for s in range(j+1,keyposition[num_instance]-1):\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                        countpunctuation(sentence[:offset[0]],cp[-1])\n",
    "                    elif i == 2:\n",
    "                        countpunctuation(stc[:ofs],cp[-1])\n",
    "                        cp[-1][2] = cp[-1][2]+1\n",
    "                        for s in range(j):\n",
    "                            cp[-1][2] = cp[-1][2]+1\n",
    "                            countpunctuation(instance[num_instance][i][s],cp[-1])\n",
    "                    else:\n",
    "                        countpunctuation(sentence[ofs:offset[0]],cp[-1])\n",
    "                        \n",
    "    return dist,cp\n",
    "\n",
    "dst = []\n",
    "feature = []\n",
    "order = []\n",
    "for i in range(len(entities_out)):\n",
    "    dst.append(distance(instance,entities_out,i,keyposition))\n",
    "    feature.append(dst[i][0][:])\n",
    "    order.append(feature[i][:])\n",
    "    temp = sorted(range(len(order[i])), key=lambda k: order[i][k])\n",
    "    n = 0\n",
    "    for obj in temp:\n",
    "        order[i][obj].append(n)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detailparagraph(instance,num_instance,entities,keyposition):\n",
    "    dp = []\n",
    "    keyline = keyposition[num_instance]\n",
    "    offset = 10 - keyline\n",
    "    numlist = countwordperline(instance,num_instance,keyposition)\n",
    "    for i in range(45):\n",
    "        dp.append(0)\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities[num_instance][i])):\n",
    "            num = instance[num_instance][i][j].count('\"')\n",
    "            num = num/2+num%2\n",
    "            if i == 1:\n",
    "                dp[3*(j+offset)+1] += num\n",
    "            elif i == 0:\n",
    "                dp[28] += num\n",
    "            elif i == 2:\n",
    "                dp[30+3*j+1] += num\n",
    "            for obj in entities[num_instance][i][j]:\n",
    "                if i == 1:\n",
    "                    dp[3*(j+offset)] += 1\n",
    "                elif i == 0:\n",
    "                    dp[27] += 1\n",
    "                elif i == 2:\n",
    "                    dp[30+3*j] += 1\n",
    "    for i in range(len(numlist)):\n",
    "        dp[2+3*(i+offset)] = numlist[i]\n",
    "    return dp\n",
    "                \n",
    "dp = []\n",
    "for i in range(len(entities_out)):\n",
    "    dp.append(detailparagraph(instance,i,entities_out,keyposition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findsimilarname(entities_i):\n",
    "    names = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities_i[i])):\n",
    "            for obj in entities_i[i][j]:\n",
    "                names.append(obj+[i,j])\n",
    "                \n",
    "    for obj in names:\n",
    "        if obj[0].find(' ') == -1:\n",
    "            bad = 0\n",
    "            for obj2 in names:\n",
    "                if obj[0].lower() in obj2[0].lower():\n",
    "                    for obj3 in names:\n",
    "                        if obj[0].lower() in obj3[0].lower():\n",
    "                            part2 = obj2[0].lower().split(' ')\n",
    "                            part3 = obj3[0].lower().split(' ')\n",
    "                            not_match = 0\n",
    "                            for t in part2:\n",
    "                                if t not in part3 and part2.index(t) != 0:\n",
    "                                    not_match = 1\n",
    "                            for t in part3:\n",
    "                                if t not in part2 and not_match == 1 and part3.index(t) != 0:\n",
    "                                    bad = 1\n",
    "            if bad == 1:\n",
    "                names.remove(obj)         \n",
    "                            \n",
    "    \n",
    "    names_new = []\n",
    "    for obj in names:\n",
    "        temp = obj[0].split(' ')\n",
    "        for obj2 in names:\n",
    "            if obj != obj2:\n",
    "                for t in temp:\n",
    "                    if ' '+t.lower()+' ' in ' '+obj2[0].lower()+' ' and ((temp.index(t) != 0  and len(temp) > 1) or (len(temp) == 1)):\n",
    "                        tmp = obj2[0].lower().split(' ')\n",
    "                        if len(temp) == 2 and len(tmp) == 2 and (t.lower() == tmp[1] and tmp[0] not in [x.lower() for x in temp]):\n",
    "                            continue\n",
    "                        else:\n",
    "                            found = 0\n",
    "                            for i in range(len(names_new)):\n",
    "                                if obj in names_new[i] and obj2 not in names_new[i]:\n",
    "                                    names_new[i].append(obj2)\n",
    "                                    found = 1\n",
    "                                elif obj2 in names_new[i] and obj not in names_new[i]:\n",
    "                                    names_new[i].append(obj)\n",
    "                                    found = 1\n",
    "                                elif obj2 in names_new[i] and obj in names_new[i]:\n",
    "                                    found = 1\n",
    "                                    pass\n",
    "                            if found == 0:\n",
    "                                names_new.append([obj,obj2])\n",
    "    for obj in names_new:\n",
    "        for obj2 in obj:\n",
    "            for obj3 in names_new:\n",
    "                if obj != obj3:\n",
    "                    if obj2 in obj3:\n",
    "                        if len(obj) >= len(obj3):\n",
    "                            names_new.remove(obj3)\n",
    "                        else:\n",
    "                            try:\n",
    "                                names_new.remove(obj)\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "    return names_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nameoccurrence(entities,keyposition):\n",
    "    nocc = []\n",
    "    for k in range(len(entities)):\n",
    "        nocc.append([])\n",
    "        names_new = findsimilarname(entities[k])\n",
    "        key = keyposition[k]\n",
    "        filling = 10 - key\n",
    "        for m in range(len(names_new)):\n",
    "            pl = []\n",
    "            for l in range(15):\n",
    "                pl.append(0)\n",
    "            for obj2 in names_new[m]:\n",
    "                if obj2[-2] == 0:\n",
    "                    pl[9] += 1\n",
    "                elif obj2[-2] == 1:\n",
    "                    pl[obj2[-1]+filling] += 1\n",
    "                elif obj2[-2] == 2:\n",
    "                    pl[10+obj2[-1]] += 1\n",
    "            names_new[m].append(pl)\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(len(entities[k][i])):\n",
    "                for obj in entities[k][i][j]:\n",
    "                    num = 1\n",
    "                    for obj2 in names_new:\n",
    "                        if obj+[i,j] in obj2:\n",
    "                            num = len(obj2)-1\n",
    "                            res = [num]+obj2[-1]\n",
    "                    if num == 1:\n",
    "                        pl = []\n",
    "                        for l in range(15):\n",
    "                            pl.append(0)\n",
    "                        if i == 0:\n",
    "                            pl[9] += 1\n",
    "                        elif i == 1:\n",
    "                            pl[j+filling] += 1\n",
    "                        elif i == 2:\n",
    "                            pl[10+j] += 1\n",
    "                        res = [num]+pl\n",
    "                    nocc[-1].append(res)\n",
    "    return nocc\n",
    "\n",
    "nocc = nameoccurrence(entities_out,keyposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformresult(result_new,position,instance):\n",
    "    result = []\n",
    "    for i in range(len(instance)):\n",
    "        result.append([])\n",
    "        for j in range(3):\n",
    "            result[-1].append([])\n",
    "            for k in range(len(instance[i][j])):\n",
    "                result[-1][-1].append([])\n",
    "                if j == 0:\n",
    "                    result[-1][-1][-1] = result_new[position[i]]\n",
    "                elif j == 1:\n",
    "                    result[-1][-1][-1] = result_new[position[i]-(len(instance[i][j])-k)]\n",
    "                elif j == 2:\n",
    "                    result[-1][-1][-1] = result_new[position[i]+k+1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### the next part is for treating the target text ####\n",
    "f = open('./target_text')\n",
    "instance_target = []\n",
    "temp = []\n",
    "p = 0\n",
    "keyposition_target = []\n",
    "line_position = []\n",
    "for line in f:\n",
    "    if '###############' in line:\n",
    "        keyposition_target.append(p)\n",
    "        current = [temp.pop()]\n",
    "        previous = temp[:]\n",
    "        temp = []\n",
    "        line_position.append(int(line.split('\\r\\n')[0].replace('#','')))\n",
    "        continue\n",
    "    if 'speaker:' in line:\n",
    "        instance_target.append([current,previous,temp])\n",
    "        temp = []\n",
    "        p = 0\n",
    "        continue\n",
    "    if line.replace(' ','') != '\\r\\n':\n",
    "        temp.append(line.replace('\\r\\n',''))\n",
    "        p = p+1\n",
    "    \n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "result_target_new = treatresultparser_new('./result_parser_target_new.json')\n",
    "result_target = transformresult(result_target_new,line_position,instance_target)\n",
    "entities_target = ner(instance_target,result_target,keyposition_target)\n",
    "for i in range(len(entities_target)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(entities_target[i][j])):\n",
    "            entities_target[i][j][k].sort(key=itemgetter(2))\n",
    "entities_target_new = entities_target[:]\n",
    "for i in range(len(entities_target)):\n",
    "    for j in range(3):\n",
    "        for k in range(len(instance_target[i][j])):\n",
    "            entities_target_new[i][j][k] = contiguous(instance_target,entities_target[i][j][k],i,j,k)\n",
    "entities_target_out,entities_target_in = findentities_out(entities_target_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word,CharacterOffsetBegin,PartOfSpeech,NamedEntityTag,Lemma,case,line\n",
    "def analyseparser(result,num_instance,instance,entities):\n",
    "    POS = []\n",
    "    for i in [1,0,2]:\n",
    "        for j in range(len(result[num_instance][i])):\n",
    "            for k in range(len(result[num_instance][i][j][1])):\n",
    "                temp = []\n",
    "                entity = result[num_instance][i][j][1][k].split('\"')[1]\n",
    "                info = ast.literal_eval(result[num_instance][i][j][1][k].split(', ',1)[1].replace(']',''))\n",
    "                offset = convertoffsetglobal(int(info['CharacterOffsetBegin']),num_instance,i,j,instance)\n",
    "                temp = [entity,offset,info['PartOfSpeech'],info['NamedEntityTag'],info['Lemma']]\n",
    "                POS.append(temp)\n",
    "                temp = []\n",
    "                part = result[num_instance][i][j][0][k].split('] [')\n",
    "                for l in range(len(part)):\n",
    "                    info = part[l].split(' ')\n",
    "                    entity = ''\n",
    "                    CharacterOffsetBegin = ''\n",
    "                    PartOfSpeech = ''\n",
    "                    Lemma = ''\n",
    "                    NamedEntityTag = ''\n",
    "                    for obj in info:\n",
    "                        if '[Text=' in obj:\n",
    "                            entity = obj[6:]\n",
    "                        elif 'Text=' in obj:\n",
    "                            entity = obj[5:]\n",
    "                        if 'CharacterOffsetBegin=' in obj:\n",
    "                            CharacterOffsetBegin = int(obj[21:])\n",
    "                        if 'PartOfSpeech=' in obj:\n",
    "                            PartOfSpeech = obj[13:]\n",
    "                        if 'Lemma=' in obj:\n",
    "                            Lemma = obj[6:]\n",
    "                        if 'NamedEntityTag=' in obj:\n",
    "                            NamedEntityTag = obj[15:]\n",
    "                    if CharacterOffsetBegin == '':\n",
    "                        continue\n",
    "                    offset = convertoffsetglobal(CharacterOffsetBegin,num_instance,i,j,instance)\n",
    "                    temp = [entity,offset,PartOfSpeech,NamedEntityTag,Lemma]\n",
    "                    POS.append(temp)\n",
    "    return POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#feature: 5 words near the candidate: 1 expression verb, 2 punctuation mark, 3 person, 4 newline, 5 nothing, 0 other cases\n",
    "def neighbourhood(instance,num_instance,entities,result):\n",
    "    f = open('./speechverbs')\n",
    "    speechverb = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            speechverb.append(line.split('\\n')[0].lower())\n",
    "    DISTANCE = 5\n",
    "    text = ''\n",
    "    nb = []\n",
    "    for i in [1,0,2]:\n",
    "        for obj in instance[num_instance][i]:\n",
    "            text += obj+'\\n'\n",
    "    analyse = analyseparser(result,num_instance,instance,entities)\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities[num_instance][i])):\n",
    "            for obj in entities[num_instance][i][j]:\n",
    "                temp = []\n",
    "                offset = obj[1]\n",
    "                offset = convertoffsetglobal(offset,num_instance,i,j,instance)\n",
    "                list1 = [(m.group(0), m.start(0)) for m in re.finditer(r\"[\\w]+|[.]+|[\\w]+[-][\\w]+|[\"+string.punctuation+\"]|[\\n]\", text[:offset])]\n",
    "                list2 = [(m.group(0), m.start(0)+offset) for m in re.finditer(r\"[\\w]+|[.]+|[\\w]+[-][\\w]+|[\"+string.punctuation+\"]|[\\n]\", text[offset:])] \n",
    "                if len(list1) >= DISTANCE:\n",
    "                    previous = list1[-DISTANCE:]\n",
    "                elif len(list1) < DISTANCE:\n",
    "                    previous = list1[-len(list1):]\n",
    "                if len(list2) > DISTANCE:\n",
    "                    following = list2[1:DISTANCE+1]\n",
    "                elif len(list2) <= DISTANCE:\n",
    "                    following = list2[1:len(list2)]\n",
    "                filling = DISTANCE-len(previous)\n",
    "                for k in range(filling):\n",
    "                    temp.append(5)\n",
    "                for part in previous + following:\n",
    "                    writen = 0\n",
    "                    if part[0] == '\\n':\n",
    "                        temp.append(4)\n",
    "                        writen = 1\n",
    "                    num = 0\n",
    "                    for c in part[0]:\n",
    "                        if c in string.punctuation:\n",
    "                            num += 1\n",
    "                    if num == len(part[0]) and writen != 1:\n",
    "                        temp.append(2)\n",
    "                        writen = 1\n",
    "                    for elem in analyse:\n",
    "                        if elem[1] == part[1] and elem[0] == part[0]:\n",
    "                            if writen != 1 and (elem[2] == 'NNP' or elem[3] == 'PERSON'):\n",
    "                                temp.append(3)\n",
    "                                writen = 1\n",
    "                            if writen != 1 and ('VB' in elem[2] and elem[4].lower() in speechverb):\n",
    "                                temp.append(1)\n",
    "                                writen = 1\n",
    "                        elif elem[1] == part[1] and elem[0] != part[0]:\n",
    "                            pass\n",
    "                    if writen == 0:\n",
    "                        temp.append(0)\n",
    "                nb.append(temp)\n",
    "                filling = DISTANCE-len(following)\n",
    "                for k in range(filling):\n",
    "                    temp.append(5)\n",
    "    f.close()\n",
    "    return nb\n",
    "\n",
    "#nb = []\n",
    "#for i in range(len(instance)):            \n",
    "#    nb.append(neighbourhood(instance,i,entities_out,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countentities(instance,entities):\n",
    "    ce = []\n",
    "    for i in range(len(entities)):\n",
    "        ce.append([])\n",
    "        offset_quote = convertoffsetglobal(instance[i][0][0].index('\"'),i,0,0,instance)\n",
    "        for j in range(3):\n",
    "            for k in range(len(entities[i][j])):\n",
    "                for obj in entities[i][j][k]:\n",
    "                    num = 0\n",
    "                    offset = obj[3]\n",
    "                    ce[-1].append([])\n",
    "                    for l in range(3):\n",
    "                        for m in range(len(entities[i][l])):\n",
    "                            for obj2 in entities[i][l][m]:\n",
    "                                if (obj2[3]-offset)*(obj2[3]-offset_quote) <0:\n",
    "                                    num += 1\n",
    "                    ce[-1][-1].append(num)\n",
    "    return ce\n",
    "\n",
    "ce = countentities(instance,entities_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing: need to combine those quotes with multilines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### for function tokenize\n",
    "def findcase(token,pos,name_similar):\n",
    "    if '<C' in token:\n",
    "        sublist = [int(token[2:-1]),pos]\n",
    "        for obj in name_similar:\n",
    "            temp = 1\n",
    "            for elem in [x in name_similar for x in sublist]:\n",
    "                temp *= elem\n",
    "            if temp == 1:\n",
    "                return 1\n",
    "        return 2\n",
    "    elif '<N' in token:\n",
    "        return 3\n",
    "    elif '<P' in token:\n",
    "        return 4\n",
    "    elif '<Q' in token:\n",
    "        return 5\n",
    "    elif '<TQ' in token:\n",
    "        return 6\n",
    "    elif '<S' in token:\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(instance,entities,result):\n",
    "    f = open('./speechverbs')\n",
    "    speechverb = []\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            speechverb.append(line.split('\\n')[0].lower())\n",
    "    exclude = set(string.punctuation)\n",
    "    tk = []\n",
    "    NUM_TOKENS = 3   # number of tokens taken both sides\n",
    "    for i in range(len(instance)):\n",
    "        text = ''\n",
    "        num = 0\n",
    "        ofs = []\n",
    "        temp = 0\n",
    "        for j in [1,0,2]:\n",
    "            for k in range(len(instance[i][j])):\n",
    "                text += instance[i][j][k]+'\\n'\n",
    "                for obj in entities[i][j][k]:\n",
    "                    temp += len(obj[0])-len('<C'+str(num)+'>')\n",
    "                    ofs.append([temp,obj[3]])\n",
    "                    num += 1\n",
    "        quote_start = convertoffsetglobal(instance[i][0][0].index('\"'),i,0,0,instance)\n",
    "        quote_end = min(text[quote_start+1:].index('\\n'),text[quote_start+1:].index('\"'))+quote_start+2\n",
    "        analyse = analyseparser(result,i,instance,entities)\n",
    "        sv = []\n",
    "        for obj in analyse:\n",
    "            if 'V' in obj[2] and obj[4].lower() in speechverb and (int(obj[1]) > quote_end or int(obj[1]) < quote_start):\n",
    "                sv.append(obj)\n",
    "        text = text[:quote_start]+'<TQ>'+text[quote_end:]\n",
    "        offset = quote_end - quote_start - len('<TQ>')\n",
    "        ofs.append([offset,quote_start])\n",
    "        ofs = sorted(ofs,key=itemgetter(1))\n",
    "        for t in range(ofs.index([offset,quote_start])+1,len(ofs)):\n",
    "            ofs[t][0] += offset\n",
    "        ofs.remove([offset,quote_start])\n",
    "        for j in [2,0,1]:\n",
    "            for k in reversed(range(len(entities[i][j]))):\n",
    "                for obj in reversed(entities[i][j][k]):\n",
    "                    name_start = obj[3]\n",
    "                    name_end = obj[3] + len(obj[0])\n",
    "                    if name_start > quote_start:\n",
    "                        name_start -= offset\n",
    "                        name_end -= offset\n",
    "                    text = text[:name_start]+'<C'+str(num-1)+'>'+text[name_end:]\n",
    "                    num -= 1\n",
    "        pos = []\n",
    "        for obj in sv:\n",
    "            filling = 0\n",
    "            for j in range(len(ofs)):\n",
    "                if obj[1] < ofs[0][1]:\n",
    "                    if obj[1] > quote_end:\n",
    "                        verb_start = obj[1] - offset\n",
    "                        verb_end = obj[1]+len(obj[0]) - offset\n",
    "                        pos.append((verb_start,verb_end))\n",
    "                        break\n",
    "                    else:\n",
    "                        verb_start = obj[1]\n",
    "                        verb_end = obj[1]+len(obj[0])\n",
    "                        pos.append((verb_start,verb_end))\n",
    "                        break\n",
    "                elif obj[1] < ofs[j][1]:\n",
    "                    if ofs[j][1] > quote_end and ofs[j-1][1] < quote_start and obj[1] > quote_end:\n",
    "                        verb_start = obj[1]-ofs[j-1][0] - offset\n",
    "                        verb_end = obj[1]-ofs[j-1][0]+len(obj[0]) - offset\n",
    "                        pos.append((verb_start,verb_end))\n",
    "                        break\n",
    "                    else:\n",
    "                        verb_start = obj[1]-ofs[j-1][0]\n",
    "                        verb_end = obj[1]-ofs[j-1][0]+len(obj[0])\n",
    "                        pos.append((verb_start,verb_end))\n",
    "                        break\n",
    "        for obj in reversed(pos):\n",
    "            text = text[:obj[0]]+'<S>'+text[obj[1]:]\n",
    "        text = text.replace('\\n','<N>')\n",
    "        pos = []\n",
    "        num = 0\n",
    "        locked = 0\n",
    "        mark = 0\n",
    "        for c in text:\n",
    "            if c == '\"' and locked == 0:\n",
    "                locked = 1\n",
    "                start = num\n",
    "            elif c == '\"' and locked == 1:\n",
    "                locked = 0\n",
    "                end = num\n",
    "                pos.append((start,end))\n",
    "            elif c == '<' and locked == 1:\n",
    "                mark = 1\n",
    "            elif c == 'N' and mark == 1:\n",
    "                end = num\n",
    "                locked = 0\n",
    "                mark = 0\n",
    "                pos.append((start,end))\n",
    "            elif mark == 1:\n",
    "                mark = 0\n",
    "            num += 1\n",
    "        for obj in reversed(pos):\n",
    "            text = text[:obj[0]]+'<Q>'+text[obj[1]+1:]\n",
    "        for c in string.punctuation:\n",
    "            if c != '<' and c != '>' and c != '.':\n",
    "                text=text.replace(c,'<P>')\n",
    "        pos = []\n",
    "        num = 0\n",
    "        locked = 0\n",
    "        for c in text:\n",
    "            if c == '<' and locked == 0:\n",
    "                locked = 1\n",
    "                temp = num\n",
    "            elif c == '<' and locked == 1:\n",
    "                pos.append(temp)\n",
    "            elif c == '>' and locked == 1:\n",
    "                locked = 0\n",
    "            elif c == '>' and locked == 0:\n",
    "                pos.append(num)\n",
    "            num += 1\n",
    "        for obj in reversed(pos):\n",
    "            text = text[:obj]+'<P>'+text[obj+1:]\n",
    "        text = re.sub(r\"[.]+\", '<P>', text)\n",
    "        name_similar = findsimilarname(entities[i])\n",
    "        order = []\n",
    "        for j in [1,0,2]:\n",
    "            for obj in entities[i][j]:\n",
    "                order += [x[3] for x in obj]\n",
    "        for j in range(len(name_similar)):\n",
    "            for k in range(len(name_similar[j])):\n",
    "                name_similar[j][k] = order.index(name_similar[j][k][3])\n",
    "        locked = 0\n",
    "        tokens = ''\n",
    "        for ch in text:\n",
    "            if ch == '<':\n",
    "                temp = ''\n",
    "                locked = 1\n",
    "            elif ch == '>':\n",
    "                locked = 0\n",
    "                tokens += '<'+temp+'> '\n",
    "            elif locked == 1:\n",
    "                temp += ch\n",
    "        tokens = tokens.split()\n",
    "        tk.append([])\n",
    "        for j in range(3):\n",
    "            for k in range(len(entities[i][j])):\n",
    "                for obj in entities[i][j][k]:\n",
    "                    temp = []\n",
    "                    pos = order.index(obj[3])\n",
    "                    for l in range(len(tokens)):\n",
    "                        if '<C'+str(pos)+'>' in tokens[l]:\n",
    "                            if l < NUM_TOKENS:\n",
    "                                filling = NUM_TOKENS - l\n",
    "                                for m in range(filling):\n",
    "                                    temp.append(0)\n",
    "                                for m in range(l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,l+NUM_TOKENS+1):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                            elif l+NUM_TOKENS >= len(tokens):\n",
    "                                filling = l+NUM_TOKENS-len(tokens)+1\n",
    "                                for m in range(l-3,l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,len(tokens)):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(filling):\n",
    "                                    temp.append(0)\n",
    "                            elif l < NUM_TOKENS and l+NUM_TOKENS >= len(tokens):\n",
    "                                filling1 = NUM_TOKENS - l\n",
    "                                filling2 = l+NUM_TOKENS-len(tokens)+1\n",
    "                                for m in range(filling1):\n",
    "                                    temp.append(0)\n",
    "                                for m in range(l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,len(tokens)):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(filling2):\n",
    "                                    temp.append(0)\n",
    "                            else:\n",
    "                                for m in range(l-3,l):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                                for m in range(l+1,l+NUM_TOKENS+1):\n",
    "                                    temp.append(findcase(tokens[m],pos,name_similar))\n",
    "                    tk[-1].append(temp)\n",
    "    f.close()\n",
    "    return tk\n",
    "tk = tokenize(instance,entities_out,result)\n",
    "#### <CN> candidate, <N> newline, <P> punctuation, <Q> quote, <TQ> target quote, <S> speech verb\n",
    "#### 0: nothing, 1: the same candidate, 2: other candidates, 3: newline, 4: punctuation, 5: other quote, 6: target quote, 7: speech verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###[1 one quotation mark or not, how many quote in it, 1 whether speaker is mentioned or not,1 whether other speaker is mentioned or not, how far from the start, the length in word]\n",
    "def quotefeature(instance,entities_out,entities_in,mode):\n",
    "    qf = []\n",
    "    if mode == 0:\n",
    "        p_0 = 0\n",
    "        p_1 = 0\n",
    "    elif mode == 1:\n",
    "        p_0 = 1\n",
    "        p_1 = -1\n",
    "    elif mode == 2:\n",
    "        p_0 = 2\n",
    "        p_1 = 0\n",
    "    for i in range(len(instance)):\n",
    "        qf.append([])\n",
    "        if len(instance[i][p_0]) == 0 or '\"' not in instance[i][p_0][p_1]:\n",
    "            for j in range(3):\n",
    "                for k in range(len(entities_out[i][j])):\n",
    "                    for obj in entities_out[i][j][k]:\n",
    "                        qf[-1].append([0,0,0,0,0,0])\n",
    "            continue\n",
    "        if instance[i][p_0][p_1].count('\"') == 1:\n",
    "            num_qm = 1\n",
    "            num_quote = 1\n",
    "        else:\n",
    "            num_quote = instance[i][p_0][p_1].count('\"')%2 + instance[i][p_0][p_1].count('\"')/2\n",
    "            num_qm = 0\n",
    "        text = instance[i][p_0][p_1]\n",
    "        text_temp = re.split(r'[^0-9A-Za-z\\']+',text[:text.index('\"')])\n",
    "        while '' in text_temp:\n",
    "            text_temp.remove('')\n",
    "        distance = len(text_temp)\n",
    "        quote = ''\n",
    "        part = instance[i][p_0][p_1].split('\"')\n",
    "        for p in range(len(part)):\n",
    "            if p%2 == 1:\n",
    "                quote += part[p]\n",
    "        text_temp = re.split(r'[^0-9A-Za-z\\']+',quote)\n",
    "        while '' in text_temp:\n",
    "            text_temp.remove('')\n",
    "        length = len(text_temp)\n",
    "        temp_qf = []\n",
    "        entities_mixed = []\n",
    "        entities_extended = []\n",
    "        for j in range(3):\n",
    "            entities_mixed.append([])\n",
    "            for k in range(len(entities_out[i][j])):\n",
    "                entities_mixed[-1].append([])\n",
    "                entities_mixed[-1][-1] = []\n",
    "                for obj in entities_out[i][j][k]:\n",
    "                    entities_extended.append(obj)\n",
    "                    entities_mixed[-1][-1].append(obj+['out'])\n",
    "            if j == 0 and k == 0:\n",
    "                temp = []\n",
    "                for obj in entities_in[i][j][k]:\n",
    "                    temp.append(obj+['in'])\n",
    "                entities_mixed[j][k] += temp\n",
    "        namegroup = findsimilarname(entities_mixed)\n",
    "        samein = []\n",
    "        for obj in namegroup:\n",
    "            samein.append([])\n",
    "            for obj2 in obj:\n",
    "                if obj2[5] == 'in':\n",
    "                    for obj3 in obj:\n",
    "                        for obj4 in entities_extended:\n",
    "                            mt = 0\n",
    "                            for l in range(4):\n",
    "                                if obj4[l] == obj3[l]:\n",
    "                                    mt += 1\n",
    "                            if mt == 4:\n",
    "                                samein[-1].append(obj4)\n",
    "        while [] in samein:\n",
    "            samein.remove([])\n",
    "        for j in range(3):\n",
    "            for k in range(len(entities_out[i][j])):\n",
    "                for obj in entities_out[i][j][k]:\n",
    "                    found = 0\n",
    "                    found_else = 0\n",
    "                    for obj2 in namegroup:\n",
    "                        for obj3 in obj2:\n",
    "                            match = 0\n",
    "                            for l in range(4):\n",
    "                                if obj[l] == obj3[l]:\n",
    "                                    match += 1\n",
    "                            if match == 4:\n",
    "                                for obj4 in obj2:\n",
    "                                    if obj4[5] == 'in':\n",
    "                                        found = 1\n",
    "                    rest = len(samein)-found\n",
    "                    if rest == 0:\n",
    "                        found_else = 0\n",
    "                    else:\n",
    "                        found_else = 1\n",
    "                    temp_qf = [num_qm,num_quote,found,found_else,distance,length]\n",
    "                    qf[-1].append(temp_qf)\n",
    "    return qf\n",
    "\n",
    "qf = quotefeature(instance,entities_out,entities_in,0)\n",
    "qf_1 = quotefeature(instance,entities_out,entities_in,1)\n",
    "qf_2 = quotefeature(instance,entities_out,entities_in,2)\n",
    "for i in range(len(qf)):\n",
    "    for j in range(len(qf[i])):\n",
    "        qf[i][j] += qf_1[i][j] + qf_2[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(feature)):\n",
    "    for j in range(len(feature[i])):\n",
    "        feature[i][j] += dst[i][1][j]\n",
    "        feature[i][j] += dp[i]\n",
    "        #feature[i][j] += nb[i][j]\n",
    "        feature[i][j] += ce[i][j]\n",
    "        feature[i][j] += nocc[i][j]\n",
    "        feature[i][j] += tk[i][j]\n",
    "        feature[i][j] += qf[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "position_fact = []\n",
    "for i in range(len(entities_out)):\n",
    "    pos = 0\n",
    "    if fact[i][0] == 0:\n",
    "        pos = fact[i][2]\n",
    "        position_fact.append(pos)\n",
    "    elif fact[i][0] == 1:\n",
    "        pos += len(entities_out[i][0][0])\n",
    "        for j in range(fact[i][1]):\n",
    "            pos += len(entities_out[i][1][j])\n",
    "        pos += fact[i][2]\n",
    "        position_fact.append(pos)\n",
    "    elif fact[i][0] == 2:\n",
    "        pos += len(entities_out[i][0][0])\n",
    "        for j in range(len(entities_out[i][1])):\n",
    "            pos += len(entities_out[i][1][j])\n",
    "        for j in range(fact[i][1]):\n",
    "            pos += len(entities_out[i][2][j])\n",
    "        pos += fact[i][2]\n",
    "        position_fact.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "for obj in feature:\n",
    "    X += obj\n",
    "Y = []\n",
    "for i in range(len(feature)):\n",
    "    temp = []\n",
    "    for j in range(len(feature[i])):\n",
    "        if j == position_fact[i]:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    Y += temp\n",
    "clf = RandomForestClassifier(n_estimators=250)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findname(index,num_instance):\n",
    "    counter = 0\n",
    "    temp_i = 0\n",
    "    temp_j = 0\n",
    "    temp_counter = 0\n",
    "    for i in range(3):\n",
    "        for j in range(len(entities_target_out[num_instance][i])):\n",
    "            temp_counter = counter\n",
    "            counter += len(entities_target_out[num_instance][i][j])\n",
    "            temp_j = j\n",
    "            temp_i = i\n",
    "            if index-counter < 0:\n",
    "                offset = index-temp_counter\n",
    "                return temp_i,temp_j,offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dst_target = []\n",
    "feature_target = []\n",
    "order_target = []\n",
    "for i in range(len(entities_target_out)):\n",
    "    dst_target.append(distance(instance_target,entities_target_out,i,keyposition_target))\n",
    "    feature_target.append(dst_target[i][0][:])\n",
    "    order_target.append(feature_target[i][:])\n",
    "    temp = sorted(range(len(order_target[i])), key=lambda k: order_target[i][k])\n",
    "    n = 0\n",
    "    for obj in temp:\n",
    "        order_target[i][obj].append(n)\n",
    "        n += 1\n",
    "dp_target = []\n",
    "for i in range(len(entities_target_out)):\n",
    "    dp_target.append(detailparagraph(instance_target,i,entities_target_out,keyposition_target))\n",
    "nb_target = []\n",
    "#for i in range(len(instance_target)):            \n",
    "#    nb_target.append(neighbourhood(instance_target,i,entities_target_out,result_target))\n",
    "ce_target = []\n",
    "ce_target = countentities(instance_target,entities_target_out)\n",
    "nocc_target = []\n",
    "nocc_target = nameoccurrence(entities_target_out,keyposition_target)\n",
    "tk_target = []\n",
    "tk_target = tokenize(instance_target,entities_target_out,result_target)\n",
    "qf_target = []\n",
    "qf_target = quotefeature(instance_target,entities_target_out,entities_target_in,0)\n",
    "qf_target_1 = quotefeature(instance_target,entities_target_out,entities_target_in,1)\n",
    "qf_target_2 = quotefeature(instance_target,entities_target_out,entities_target_in,2)\n",
    "for i in range(len(qf_target)):\n",
    "    for j in range(len(qf_target[i])):\n",
    "        qf_target[i][j] += qf_target_1[i][j] + qf_target_2[i][j]\n",
    "for i in range(len(feature_target)):\n",
    "    for j in range(len(feature_target[i])):\n",
    "        feature_target[i][j] += dst_target[i][1][j]\n",
    "        feature_target[i][j] += dp_target[i]\n",
    "        #feature_target[i][j] += nb_target[i][j]\n",
    "        feature_target[i][j] += ce_target[i][j]\n",
    "        feature_target[i][j] += nocc_target[i][j]\n",
    "        feature_target[i][j] += tk_target[i][j]\n",
    "        feature_target[i][j] += qf_target[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for t in range(len(feature_target)):\n",
    "    temp = []\n",
    "    for i in range(len(feature_target[t])):\n",
    "        nc,nl,n = findname(i,t)\n",
    "        temp.append([nc,nl,n,entities_target_out[t][nc][nl][n],clf.predict_proba([feature_target[t][i]])[0][1].item()])\n",
    "    res.append(sorted(temp,key=itemgetter(4))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./result.spk', 'w') as fp:\n",
    "    for obj in res:\n",
    "        json.dump(obj,fp)\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839382940109\n"
     ]
    }
   ],
   "source": [
    "print float(res.count(1))/len(res)    ### random forest 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81941923775\n"
     ]
    }
   ],
   "source": [
    "print float(res.count(1))/len(res)   ### ramdon forest 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.638838475499\n"
     ]
    }
   ],
   "source": [
    "print float(res.count(1))/len(res)   ### Ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790381125227\n"
     ]
    }
   ],
   "source": [
    "print float(res.count(1))/len(res)   ### Decision Tree  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature [1 distance in words from the quote, \n",
    "#         1 the ordinal position, \n",
    "#         6 numbers of ',', '.', '\\n', '!', '?', '\"' between name and quote,\n",
    "#         45 Number of names, quotes and words in each one of 15 paragraphs\n",
    "#         10 words of the neighbourhood (unused)\n",
    "#         1 number of entities between name and quote\n",
    "#         1 total number of appearance of this entity\n",
    "#         15 numbers of appearance of this entity in each paragraph\n",
    "#         6 numbers of tokens 3 each side of the target]\n",
    "\n",
    "#         1 this name is the subject or object of speech verb\n",
    "#         9 speakers of 9 previous paragraphs \n",
    "#         10-fold cross validation(to be continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part 2  coreference resolution\n",
    "# require to load function treatresultparser_new\n",
    "f = './result_parser_target_new.json'\n",
    "result_target_new = treatresultparser_new(f)\n",
    "keyword = ['mother','father','son','daughter','uncle','aunt','child','brother','sister'\n",
    "          ,'cousin','wife','husband','niece','nephew','grandfather','grandmother']\n",
    "# his/her/its... ,adj,  keyword, name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_token = []\n",
    "for obj in result_target_new:\n",
    "    result_token.append([])\n",
    "    for i in range(2):\n",
    "        if i == 1:\n",
    "            for j in range(len(obj[i])):\n",
    "                temp = ast.literal_eval(obj[i][j].split(', ',1)[1][:-2])\n",
    "                result_token[-1].append([obj[i][j].split(', ',1)[0].replace('\"',''),int(temp['CharacterOffsetBegin']),int(temp['CharacterOffsetEnd']),temp['PartOfSpeech']])\n",
    "        if i == 0:\n",
    "            for j in range(len(obj[i])):\n",
    "                if '] [' in obj[i][j]:\n",
    "                    temp = obj[i][j].split('] [')\n",
    "                    for k in range(len(temp)):\n",
    "                        result_token[-1].append([temp[k].split('Text=')[1].split(' ')[0],int(temp[k].split('CharacterOffsetBegin=')[1].split(' ')[0]),int(temp[k].split('CharacterOffsetEnd=')[1].split(' ')[0]),temp[k].split('PartOfSpeech=')[1].split(' ')[0]])\n",
    "\n",
    "for i in range(len(result_token)):\n",
    "    result_token[i] = sorted(result_token[i],key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print result_token[5]\n",
    "info_pattern = []\n",
    "for i in range(len(result_token)):\n",
    "    info_pattern.append([])\n",
    "    temp = ''\n",
    "    stat = 0\n",
    "    for obj2 in result_token[i]:\n",
    "        if obj2[3] == 'PRP$' and stat == 0:\n",
    "            stat = 1\n",
    "            temp += obj2[0]+' '\n",
    "        elif obj2[3] == 'JJ' and stat != 0:\n",
    "            temp += obj2[0] +' '\n",
    "        elif obj2[0].lower() in keyword and stat == 1:\n",
    "            stat = 2\n",
    "            temp += obj2[0]+' '\n",
    "        elif 'NNP' in obj2[3] and stat == 2:\n",
    "            temp += obj2[0]+' '\n",
    "            info_pattern[-1].append(temp)\n",
    "            stat = 3\n",
    "        elif 'NNP' in obj2[3] and stat == 3:\n",
    "            temp += obj2[0]+' '\n",
    "            info_pattern[-1].pop()\n",
    "            info_pattern[-1].append(temp)\n",
    "        else:\n",
    "            temp = ''\n",
    "            stat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('./text')\n",
    "raw_text = []\n",
    "for line in f:\n",
    "    if line.replace(' ','')!='\\r\\n' and line!='\\n':\n",
    "        raw_text.append(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_coref = []\n",
    "for i in range(len(info_pattern)):\n",
    "    if info_pattern[i] != []:\n",
    "        for j in range(len(info_pattern[i])):\n",
    "            temp = ''\n",
    "            if i < 2:\n",
    "                for t in range(i+1):\n",
    "                    temp += raw_text[t]\n",
    "                temp = temp.replace('\\r\\n','')\n",
    "            elif i >= 2:\n",
    "                for t in [-2,-1,0]:\n",
    "                    temp += raw_text[i+t]\n",
    "                temp = temp.replace('\\r\\n','')\n",
    "            if '\"' in temp:\n",
    "                temp2 = []\n",
    "                for m in re.finditer('\"', temp):\n",
    "                     temp2.append(m.start())\n",
    "                pos = temp.index(info_pattern[i][j][:-1])\n",
    "                for t in range(len(temp2)):\n",
    "                    if pos < temp2[t]:\n",
    "                        pos2 = t-1\n",
    "                        break\n",
    "                    pos2 = t\n",
    "                temp3 = temp.split('\"')\n",
    "                temp = ''\n",
    "                if pos2%2 == 1:\n",
    "                    for k in range(len(temp3)):\n",
    "                        if k%2 == 0:\n",
    "                            temp += temp3[k]\n",
    "                elif pos2%2 == 0:\n",
    "                    if pos2 == len(temp2)-1:\n",
    "                        for k in range(len(temp3)):\n",
    "                            if k%2 == 0:\n",
    "                                temp += temp3[k]\n",
    "                        temp += temp3[-1]\n",
    "                    else:\n",
    "                        for k in range(len(temp3)):\n",
    "                            if k%2 == 0:\n",
    "                                temp += temp3[k]\n",
    "                            if k == pos2+1:\n",
    "                                temp += '\"'+temp3[k]+'\"'\n",
    "            text_coref.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info_pattern_new = []\n",
    "pattern_line = []\n",
    "n = 0\n",
    "for obj in info_pattern:\n",
    "    if obj != []:\n",
    "        for i in range(len(obj)):\n",
    "            pattern_line.append(n)\n",
    "            if obj[i].endswith(' '):\n",
    "                obj[i] = obj[i][:-1]\n",
    "            info_pattern_new.append(obj[i])\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info_parser = []\n",
    "server = jsonrpclib.Server(\"http://localhost:8080\")\n",
    "for obj in text_coref:\n",
    "    info_parser.append(loads(server.parse(obj)))\n",
    "info_coref = []\n",
    "for obj in info_parser:\n",
    "    try:\n",
    "        info_coref.append(obj['coref'])\n",
    "    except KeyError:\n",
    "        info_coref.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analysecoreference(text_coref,info_coref,info_pattern_new):\n",
    "    result = []\n",
    "    for i in range(len(text_coref)):\n",
    "        num_sent = 0\n",
    "        num_token = 0\n",
    "        temp = tokenizer.tokenize(text_coref[i])\n",
    "        for p in reversed(range(len(temp))):\n",
    "            if '.  ' in temp[p] and temp[p][temp[p].index('.  ')-1]!='.':\n",
    "                stock = temp[p].split('.  ',1)\n",
    "                temp[p] = stock[0]+'.'\n",
    "                temp.insert(p+1,stock[1])\n",
    "            elif temp[p].endswith('...') and p!= len(temp)-1:\n",
    "                temp[p] += '  '+temp[p+1]\n",
    "                temp.pop(p+1)\n",
    "        for j in range(len(temp)):\n",
    "            if info_pattern_new[i] in temp[j]:\n",
    "                num_sent = j\n",
    "                temp2 = word_tokenize(temp[j])\n",
    "                pattern_split = word_tokenize(info_pattern_new[i])\n",
    "                pattern_new = ''\n",
    "                for obj in pattern_split:\n",
    "                    pattern_new += obj\n",
    "                for p in range(len(temp2)-2):\n",
    "                    t = temp2[p] + temp2[p+1] + temp2[p+2]\n",
    "                    if t in pattern_new:\n",
    "                        num_token = p\n",
    "                        break\n",
    "        for obj in info_coref[i]:\n",
    "            for obj2 in obj:\n",
    "                for obj3 in obj2:\n",
    "                    if word_tokenize(info_pattern_new[i])[0] == obj3[0] and obj3[1] == num_sent and obj3[3] == num_token:\n",
    "                        result.append([obj,pattern_line[i],i])\n",
    "    return result\n",
    "\n",
    "result_coref = analysecoreference(text_coref,info_coref,info_pattern_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relation_object = []\n",
    "for obj in info_pattern_new:\n",
    "    relation = ''\n",
    "    position_relation = 0\n",
    "    o = ''\n",
    "    temp = word_tokenize(obj)\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] in keyword:\n",
    "            position_relation = i\n",
    "            relation = temp[i]\n",
    "    for j in range(position_relation+1,len(temp)):\n",
    "        o += temp[j]+' '\n",
    "    o = o[:-1]\n",
    "    relation_object.append([relation,o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relation_person = []\n",
    "for i in range(len(result_coref)):\n",
    "    tokens = result_token[result_coref[i][1]-2] + result_token[result_coref[i][1]-1] + result_token[result_coref[i][1]]\n",
    "    for obj2 in result_coref[i][0]:\n",
    "        for obj3 in obj2:\n",
    "            temp = word_tokenize(obj3[0])\n",
    "            for obj4 in temp:\n",
    "                for obj5 in tokens:\n",
    "                    if obj5[0] == obj4 and obj5[3] == 'NNP' and [obj3[0],relation_object[result_coref[i][2]][0],relation_object[result_coref[i][2]][1]] not in relation_person:\n",
    "                        relation_person.append([obj3[0],relation_object[result_coref[i][2]][0],relation_object[result_coref[i][2]][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eddard is Brandon Stark's brother\n",
      "Eddard is Torrhen Karstark's brother\n",
      "Benjen is his brother Benjen instead of that ragged stranger's brother\n",
      "Robb is Jon's brother\n",
      "Theon is Ned's son\n",
      "Jaime is Robert 's queen's brother\n",
      "Jaime is Cersei Lannister's brother\n",
      "Ser Brynden is Lysa Arryn's brother\n",
      "Ser Edmure is Lysa Arryn's son\n",
      "Stannis is Joffrey's uncle\n",
      "Stannis is Joffrey on his throne's uncle\n",
      "Rhaegar is Rhaegar's brother\n",
      "Lyanna is Lyanna's daughter\n",
      "Lyanna is me , another Brandon , my father 's brother's daughter\n",
      "Brandon is Lyanna's son\n",
      "Brandon is me , another Brandon , my father 's brother's son\n",
      "Arya is Jon Snow's sister\n",
      "Arya is Jon's sister\n",
      "Rhaegar is Drogo 's's brother\n",
      "Rhaegar is Khal Drogo's brother\n",
      "Rhaegar is Drogo's brother\n",
      "Edmure is Catelyn's brother\n",
      "Ser Perwyn is Ser Stevron's brother\n",
      "Ser Brynden is Catelyn's uncle\n",
      "Ben is Jon's uncle\n",
      "Jaime is Tyrion's brother\n",
      "Jaime is TYRION the Lord Commander's brother\n",
      "Maege is Mormont's sister\n",
      "Jaime is Tyrion Lannister's brother\n",
      "Sansa is Pycelle's daughter\n",
      "Brandon is old Lord Rickard's son\n",
      "Brandon is Rickard's son\n",
      "Torrhen is his father 's namesake Eddard's brother\n",
      "Stannis is The Usurper's brother\n",
      "Gregor is Ser Aron Santagar's brother\n",
      "Jaime is Joffrey's uncle\n",
      "Jaime is Bronn's brother\n",
      "Brandon is Nan's uncle\n",
      "Brandon is Old Nan's uncle\n",
      "Arya is Maester Aemon's sister\n",
      "Jaime is Lord Tywin's son\n",
      "Arya is Ned's daughter\n",
      "Rhaegar is Daenerys's brother\n",
      "Rhogoro is Khal Moro's son\n",
      "Ser Wendel is Catelyn's brother\n",
      "Wendel is the Manderly banners-the white merman with trident in hand , rising from a blue-green sea-and's brother\n",
      "Cersei is Tyrion Lannister's sister\n",
      "Arya is Sansa's sister\n",
      "Arya is a good girl , Sansa's sister\n",
      "Joffrey is Ned's son\n",
      "Renly is Renly's brother\n",
      "Jaime is Tywin Lannister 's's son\n",
      "Jaime is Lord Tywin 's's son\n",
      "Kevan is Lord Lefford's uncle\n",
      "Lysa is Catelyn's sister\n",
      "Jaime is Tyrion 's's brother\n",
      "Brynden is Catelyn's uncle\n",
      "Benjen is Eddard Stark's brother\n",
      "Jaime is Lysa Arryn's brother\n",
      "Jaime is Lysa's brother\n",
      "Bran is Catelyn Stark's son\n",
      "Edmure is Littlefinger's brother\n",
      "Maegor is Aegon the Conqueror's son\n",
      "Viserys is Dany's brother\n",
      "Viserys is Dany for bride gifts's brother\n",
      "Stannis is Lord Renly's brother\n",
      "Brandon is Ned's brother\n",
      "Brandon is Ned with a smile on his lips that bordered on insolence's brother\n"
     ]
    }
   ],
   "source": [
    "for obj in relation_person:\n",
    "    print obj[2]+' is '+obj[0]+'\\'s '+obj[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
